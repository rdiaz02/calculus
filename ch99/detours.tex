%%chapter%% 99
\chapter{Detours}

%\newcommand{\detour}[2]{\textbf{#2}\label{detour:#1}}
\newcommand{\detour}[2]{\subsection{#2}\label{detour:#1}}

\vfill

\detour{def-tangent}{Formal definition of the tangent line}\index{tangent line!formal definition}

Given a function $x(t)$, consider any point $\zu{P}=(a,x(a))$ on its graph.
Let the function $\ell(t)$ be a line passing through P.
We say that $\ell$ cuts through $x$ at P if
there exists some real number $d>0$ such that
the graph of $\ell$ is on one side of the graph of $x$ for
all $a-d < t < a$, and is on the other side for all $a < t < a+d$.

Definition (Marsden\footnote{\emph{Calculus Unlimited}, by Jerrold
Marsden and Alan Weinstein, \verb@http://resolver.caltech.edu/CaltechBOOK:1981.001@}): A line $\ell$ through P is said to be the  line tangent to $x$ at P if all
lines through P with slopes less than that of $\ell$ cut through $x$
in one direction, while all lines with slopes greater than P's cut through
it in the opposite direction.

The reason for the complication in the definition is that 
there are cases in which the function is smooth and well-behaved throughout a certain
region, but for a certain point P in that region,
all lines through P cut through P.
For example, the function $x(t)=t^3$ is blessed everywhere with lines that don't
cut through it --- everywhere, that is,
except at $t=0$,
which is an inflection point (p. \pageref{inflection}).
Our definition fills in the
``gap tooth'' in the derivative function in the obvious way.

\begin{eg}
As an example, we demonstrate that the derivative of $t^3$ is zero where it passes through the origin.
Define the line $\ell(t)=bt$ with slope $b$, passing
through the origin. For $b<0$, $\ell$ cuts the graph of $t^3$ once at the origin, going down and to the right.
For $b>0$, $\ell$ cuts the graph of $t^3$ in three places, at $t=0$ and $\pm\sqrt{b}$.
Picking any positive value of  $d$ less than $\sqrt{b}$,
we find that $\ell$ cuts the graph at the origin, going up and to the right. Therefore
$b=0$ gives the tangent line at the origin.
\end{eg}


\pagebreak

\detour{polynomial-proof}{Derivatives of polynomials}\index{derivative!of a polynomial}

Some ideas in this proof are due to Tom Goodwillie.

Theorem: For $n=0$, 1, 2, \ldots, the derivative of the function $x$ defined by
$x(t)=t^n$ is $\dot{x}=nt^{n-1}$.

The results for $n=0$ and 1 hold by direct application of the definition of the derivative.

For $n>1$, it suffices to
prove $\dot{x}(0)=0$ and $\dot{x}(1)=n$, since the result for other
nonzero values of $t$ then follows by the kind of scaling argument
used on page \pageref{scaling} for the $n=2$ case.

We use the following properties of the derivative, all of which follow immediately from its
definition as the slope of the tangent line:

\begin{itemize}
\item[\emph{Shift}.] Shifting a function $x(t)$ horizontally to form a new function $x(t+c)$ gives a derivative
         at any newly shifted point that is the same as the derivative at the corresponding point on the unshifted graph.
\item[\emph{Flip}.] Flipping the function $x(t)$ to form a new function $x(-t)$ negates
         its derivative at $t=0$.
\item[\emph{Add}.] The derivative of the sum or difference of two functions is the sum or difference of their derivatives.
\end{itemize}

For even $n$, $\dot{x}(0)=0$ follows from the flip property, since $x(-t)$ is the same function as $x(t)$.
For $n=3,$ 5, \ldots, we apply the definition of the derivative in the same manner as was done in the preceding
section for $n=3$.

We now need to show that $\dot{x}(1)=n$. Define the function $u$ as
\begin{align*}
  u(t) &= x(t+1)-x(t) \\
       &= 1+nt + \ldots\eqquad,
\end{align*}
where the second line follows from the binomial theorem, and \ldots represents terms involving $t^2$ and
higher powers. Since we've already established the results for $n=0$ and 1,
differentiation gives
\begin{equation*}
  \dot{u}(t) = n + \ldots\eqquad.
\end{equation*}
Now let's evaluate this at $t=0$, where, as shown earlier, the terms represented by \ldots all vanish.
Applying the add and shift properties, we have
\begin{equation*}
  \dot{x}(1)-\dot{x}(0) = n\eqquad.
\end{equation*}
But since $\dot{x}(0)=0$, this completes the proof.

Although this proof was for integer exponents $n\ge 1$, the result is also true for any
real value of $n$; see example \ref{eg:der-x-to-any-k}
on p.~\pageref{eg:der-x-to-any-k}.

\detour{sin-rigor}{Details of the proof of the derivative of the sine function}\index{derivative!of the sine}

Some ideas in this proof are due to Jerome Keisler (see references, p. \pageref{references}).

On page \pageref{eg:derivative-of-sin}, I computed the derivative of $\sin t$ to be $\cos t$
as follows:
\begin{align*}
  \der x &= \sin(t+\der t)-\sin t\eqquad, \\
         &= \sin t \: \cos \der t \\
         &\quad + \cos t \: \sin \der t - \sin t \\
         &= \cos t \der t + \ldots\eqquad.
\end{align*}
We want to prove prove that the error ``\ldots'' introduced by the small-angle approximations really
is of order $\der t^2$. 

A quick and dirty way to check whether this is likely to be true is to
use Inf to calculate $\sin(t+\der t)$ at some specific value of $t$. For example, at $t=1$ we have
this result:
\begin{Code}
  \ii : sin(1+d)
  \oo{(0.84147)+(0.54030)d}
  \oo{+(-0.42074)d^2+(-0.09006)d^3}
  \oo{+(0.03506)d^4}
\end{Code}
The small-angle approximations give $\sin(1+d)\approx\sin 1+(\cos 1)d$.
The coefficients of the first two terms of the exact result are, as expected $\sin(1)=0.84147$ and $\cos(1)=0.5403\ldots$, so although the
small-angle approximations have introduced some errors, they involve only higher powers of $\der t$, as claimed.

The demonstration with Inf has two shortcomings. One is that it only works for $t=1$, but we need to
prove that the result for all values of $t$. That doesn't mean that the check for $t=1$ was useless.
Even though a general mathematical statement about all numbers can never be \emph{proved} by demonstrating
specific examples for which it succeeds, a single counterexample suffices to \emph{disprove} it.
The check for $t=1$ was worth doing, because if the first term had come out to be
0.88888, it would have immediately disproved our claim, thereby
saving us from wasting hours attempting to prove something that wasn't true.

The other problem is that I've never explained how Inf calculates this kind of thing.
The answer is that it uses something called
a Taylor series, discussed in section \ref{sec:taylor}. Using Inf here without knowing yet how Taylor series
work is like using your calculator as a ``black box'' to extract the square root of $\sqrt{2}$ without knowing how
it does it. Not knowing the inner workings of the black box makes the demonstration less than satisfying.

In any case, this preliminary check makes it sound like it's reasonable to go on and try to produce a
real proof. We have
\begin{equation*}
  \sin(t+\der t) = \sin t + \cos t \der t - E\eqquad,
\end{equation*}
where the error $E$ introduced by the approximations is
\begin{align*}
  E = &\sin t (1-\cos \der t) \\
    + &\cos t (\der t - \sin \der t)\eqquad.
\end{align*}

\smallfig[h]{sin-rigor}{Geometrical interpretation of the error term.}
Let the radius of the circle in figure \figref{sin-rigor} be one, so AD
is $\cos \der t$ and CD is $\sin \der t$. The area of the shaded pie slice
is $\der t/2$, and the area of triangle ABC is $\sin\der t/2$, so the
error made in the approximation $\sin\der t\approx\der t$ equals twice
the area of the dish shape formed by line BC and arc BC. Therefore
$\der t-\sin\der t$ is less than the area of rectangle CEBD.
But CEBD has both an infinitesimal width and an infinitesimal height,
so this error is of no more than order $\der t^2$.

For the approximation $\cos\der t\approx 1$, the error (represented
by BD) is $1-\cos\der t=1-\sqrt{1-\sin^2\der t}$, which is less
than $1-\sqrt{1-\der t^2}$, since $\sin \der t<\der t$. Therefore
this error is of order $\der t^2$.

\detour{transfer}{Formal statement of the transfer principle}\label{transfer}
On page \pageref{backref-transfer}, I gave an informal description of the transfer principle.
The idea being expressed was that the phrases ``for any'' and
``there exists'' can only be used in phrases like ``for any real number $x$'' and ``there exists
a real number $y$ such that\ldots'' The transfer principle does not apply to statements like ``there exists
an integer $x$ such that\ldots'' or even ``there exists a subset of the real numbers such that\ldots''

The way to state the transfer principle more rigorously is to get rid of the ambiguities of
the English language by restricting ourselves to a well-defined language of mathematical symbols.
This language has symbols $\forall$ and $\exists$, meaning "for all" and "there exists," and these
are called quantifiers.\index{quantifier} A quantifier is always immediately followed by a variable, and
then by a statement involving that variable.
For example, suppose we want to say that a number greater than 1 exists. We can write the statement $\exists x \; x>1$,
read as ``there exists a number $x$ such that $x$ is greater than 1.'' We don't actually need to say
``there exists a number $x$ \emph{in the set of real numbers} such that \ldots,'' because our intention here is to make statements
that can be translated back and forth between the reals and the hyperreals. In fact, we forbid this
type of explicit reference to the domain to which the quantifiers apply. This restriction is described
technically by saying that we're only allowing \emph{first-order logic}.

Quantifiers can be nested. For example, I can state the commutativity of addition as
$\forall x \forall y \; x+y=y+x$, and the existence of additive inverses as $\forall x \exists y \; x+y=0$.

After the quantifier and the variable, we have some mathematical assertion, in which we're allowed to use the symbols
$=$, $>$, $\times$ and $+$ for the basic operations of arithmetic, and also parentheses and the logical
operators $\neg$, $\wedge$ and $\vee$ for ``not,'' ``and,'' and ``or.''
Although we will often find it convenient to use other symbols, such as 0, 1,
$-$, $/$, $\le$, $\ne$, etc., these are not strictly necesary. We use them only as a way of making the formulas more
readable, with the understanding that they could be translated into the more basic symbols.
For instance, I can restate $\exists x \; x>1$ as $\exists x \exists y \forall z \; yz=z \wedge x>y$.
The number $y$ ends up just being a name for 1, because it's the only number that will always satisfy
$yz=z$.

Finally, these statements need to satisfy certain syntactic rules. For example, we can't have a string of
symbols like $x+\times y$, because the operators $+$ and $\times$ are supposed to have numbers on both sides.

A finite string of symbols satisfying all the above rules is called a well-formed formula (wff)\index{well-formed formula}
in first-order logic.

The transfer principle states that a wff is true on the real numbers if and only if it is true on the hyperreal numbers.

If you look in an elementary algebra textbook at  the statement of all the elementary axioms of the real number system, such
as commutativity of multiplication, associativity of addition, and so on, you'll see that they can all
be expressed in terms of first-order logic, and therefore you can use them when manipulating hyperreal numbers.
However, it's not possible to fully characterize the real number system without giving at least some further
axioms that cannot be expressed in first order. There is more than one way to set up these additional axioms,
but for example one common axiom to use is the Archimedean principle,\index{Archimedean principle}
which states that there is no number that is greater than 1, greater than $1+1$, greater than $1+1+1$, and so on.
If we try to express this as a well-formed formula in first order logic, one attempt would be
$\neg \exists x \; x>1 \;\wedge\; x>1+1  \;\wedge\; x>1+1+1 \ldots$, where the $\dots$ indicates that the string of
symbols would have to go on forever. This doesn't work because a well-formed formula has to be a \emph{finite}
string of symbols. Another attempt would be $\exists x \forall n \in \mathbb{N} \; x>n$, where $\mathbb{N}$ means
the set of integers. This one also fails to be a wff in first-order logic, because in first-order logic
we're not allowed to explicitly refer to the domain of a quantifier. We conclude that the transfer principle
does not necessarily apply to the Archimedean principle, and in fact the Archimedean principle is not
true on the hyperreals, because they include numbers that are infinite.

Now that we have a thorough and rigorous understanding of what the transfer principle says, the next obvious
question is why we should believe that it's true. This is discussed in the following section.

\detour{transfer-true}{Is the transfer principle true?}

The preceding section stated the transfer principle in rigorous language. But why should we believe that it's
true?

One approach would be to begin deducing things about the hyperreals, and see if we can deduce a contradiction.
As a starting point, we can use the axioms of elementary algebra, because the transfer principle tells us that
those apply to the hyperreals as well. Since we also assume that the Archimedean principle does \emph{not} hold
for the hyperreals, we can also base our reasoning on that, and therefore many of the things we can prove
will be things that are true for the hyperreals, but false for the reals. This is essentially what mathematicians
started doing immediately after Newton and Leibniz invented the calculus, and they were immediately successful
in producing contradictions. However, they weren't using formally defined logical systems, and they hadn't
stated anything as specific and rigorous as the transfer principle. In particular, they didn't understand
the need for anything like our restriction of the transfer principle to first-order logic. If we could
reach a contradiction based on the more modern, rigorous statement of the transfer principle, that would
be a different matter. It would tell us that one of two things was true: either (1) the hyperreal number system lacks logical self-consistency,
or (2) both the hyperreals and the reals lack self-consistency.

Abraham Robinson proved, however, around 1960 that the reals and the hyperreals have the same level of
consistency: one is self-consistent if and only if the other is. In other words, if the hyperreals harbor
a ticking logical time bomb, so do the reals. Since most mathematicians don't lose much sleep worrying
about a lack of self-consistency in the real number system, this is generally taken as meaning that
infinitesimals have been rehabilitated. In fact, it gives them an even higher level of respectability than
they had in the era of Gauss and Euler, when they were widely used, but mathematicians knew a valid
style of proof involving infinitesimals only because they'd slowly developed the right ``Spidey sense.''

But how in the world could Robinson have proved such a thing? It seems like a daunting task. There is
an infinite number of possible logical trains of argument in mathematics. How could he have demonstrated, with
a stroke of a pen, that \emph{none} of them could ever lead to a contradiction (unless it indicated
a contradiction lurking in the real number system as well)? Obviously it's not possible to check
them all explicitly.

The way modern logicians prove such things is usually by using \emph{models}.\index{model}
For an easy example of a model, consider Euclidean geometry. Euclid believed that the following four
postulates\footnote{modified slightly by me from a translation by T.L. Heath, 1925} were all self-evident:

\begin{enumerate}
\item Let the following be postulated: to draw a straight line from any point to any point.
\item To extend a finite straight line continuously in a straight line.
\item To describe a circle with any center and radius.
\item That all right angles are equal to one another.
\end{enumerate}

These postulates, which today we would call ``axioms,'' played the same role with respect to Euclidean geometry that the elementary axioms of arithmetic
play for the real number system.

Euclid also found that he needed a fifth postulate in order to prove many of his most important
theorems, such as the Pythagorean theorem. I'll state a different axiom that turns out to be
equivalent to it:

\qquad 5. \emph{Playfair's version of the parallel postulate:} Given any infinite line L, and any point P not on that line,
there exists a unique infinite line through P that never crosses L.

The ancients believed this to be less obviously self-evident than the first four, partly because
if you were given the two lines, it could theoretically take an infinite amount of time to inspect
them and verify that they never crossed, even at some very distant point. Euclid avoided even mentioning
infinite lines in postulates 1-4, and he considered postulate 5 to be so much less intuitively appealing in comparison that
he organized the \emph{Elements} so that the first 28 propositions were those that could be proved
without resorting to it. Continuing the analogy with the reals and hyperreals, the parallel postulate
plays the role of the Archimedean principle: a statement about infinity that we don't feel quite so sure about.

For centuries, geometers tried to prove the parallel postulate from the first five. The trouble with
this kind of thing was that it could be difficult to tell what was a valid proof and what wasn't.
The postulates were written in an ambiguous human language, not a formal logical system.
As an example of the kind of confusion that could result,
suppose we assume the following postulate, $5'$, in place of 5:

\qquad $5'$: Given any infinite line L, and any point P not on that line,
every infinite line through P crosses L.

Postulate $5'$ plays the role for noneuclidean geometry that the negation of the Archimedean principle
plays for the hyperreals. It tells us we're not in Kansas anymore. If a geometer can start from
postulates 1-4 and $5'$ and arrive at a contradiction, then he's made significant progress toward
proving that postulate 5 has to be true based on postulates 1-4. (He would also have to disprove
another version of the postulate, in which there is more than one parallel through P.) For centuries,
there have been reasonable-sounding arguments that seemed to give such a contradiction. For instance,
it was proved that a geometry with $5'$ in it was one in which distances were limited to some finite
maximum. This would appear to contradict postulate 3, since there would be a limit on the radius
of a circle. But there's plenty of room for disagreement here, because the ancient Greeks didn't
have any notion of a set of real numbers. For them, the thing we would call a number was simply a
finite straight line (line segment) with a certain length. If postulate 3 says that we can make a circle given any radius,
it's reasonable to interpret that as a statement that \emph{given any finite straight line} as the
specification of the radius, we can make the circle. There is then no contradiction, because the too-long
radius can't be specified in the first place. This muddle is similar to the kind of confusion that
reigned for centuries after Newton: did infinitesimals lead to contradictions?

In the 19th century, Lobachevsky and Bolyai came up with a version of Euclid's axioms
that was more rigorously defined, and that was carefully engineered to avoid the kinds of
contradictions that had previously been discovered in noneuclidean geometry. This is analogous
to the invention of the transfer principle and the realization that the restriction to first-order
logic was necessary. Lobachevsky and Bolyai slaved away for year after year proving new results
in noneuclidean geometry, wondering whether they would ever reach a contradiction. Eventually they
started to doubt that there were ever going to be contradictions, and finally they proved that
the contradictions didn't exist.

The technique for proving consistency was to make a \emph{model} of the noneuclidean
system. Consider geometry done on the surface of a sphere. The word ``line'' in the axioms now
has to be understood as referring to a great circle, i.e., one with the same radius as the sphere.
The parallel postulate fails, because parallels don't exist: every great circle intersects every other great circle.
One modification has to be made to the model in order to make it consistent with the first postulate.
The constructions described in Euclid's postulates are tacitly assumed to be unique (and in more
rigorous formulations are explicitly stated to be so). We want there to be a unique line defined by
any two distinct points. This works fine on the sphere as long as the points aren't too far apart, but
it fails if the points are antipodes, i.e., they lie at opposite sides of the sphere. For example, every line of longitude on the
Earth's surface passes through both poles.
The solution to this problem is to modify what we mean by ``point.'' Points at each
other's antipodes are considered to be the \emph{same point}. (Or, equivalently, we can do geometry on
a hemisphere, but agree that when we go off one edge, we ``wrap around'' to the opposite side.)

This spherical model obeys all the postulates of this particular system of noneuclidean
geometry. But consider now that we constructed it \emph{inside} a surrounding three-dimensional
space in which the parallel postulate does hold. Now suppose we keep on proving theorems in this
system of noneuclidean geometry, filling up page after page with proofs using words like ``line,''
which we mentally associate with great circles on a certain sphere --- and eventually we reach a
contradiction. But now we can go back through our proofs, and in every place where the word
``line'' occurs we can cross it out with a red pencil and put in ``great circle on this particular
sphere.'' It would now be a proof about \emph{Euclidean} geometry, and the contradiction would
prove that \emph{Euclidean} geometry lacked self-consistency. We therefore arrive at the result
that if noneuclidean geometry is inconsistent, so is Euclidean geometry. Since nobody believes
that Euclidean geometry is inconsistent, this is considered the moral equivalent of proving
noneuclidean geometry to be consistent.

If you've been keeping the system of analogies in mind as you read this story, it should be clear
what's coming next. If we want to prove that the hyperreals have the same consistency as the
reals, we just have to construct a \emph{model} of the hyperreals using the reals. This is done
in detail elsewhere (see Stroyan and Mathforum.org in the references, p. \pageref{references}).
I'll just sketch
the general idea. A hyperreal number is represented by an infinite sequence of real numbers.
For example, the sequence
\begin{equation*}
  7, 7, 7, 7, \ldots
\end{equation*}
would be the hyperreal version of the number 7.
A sequence like
\begin{equation*}
  1, 2, 3, \ldots
\end{equation*}
represents an infinite number, while
\begin{equation*}
  1, \frac{1}{2}, \frac{1}{3}, \ldots
\end{equation*}
is infinitesimal. All the arithmetic operations are defined by applying them to the corresponding
members of the sequences. For example, the sum of the 7, 7, 7, \ldots sequence and the
1, 2, 3, \ldots sequence would be 8, 9, 10, \ldots, which we interpret as a somewhat larger
infinite number.

The big problem in this approach is how to compare hyperreals, because a comparison like $<$
is supposed to give an answer that is either true or false. It's not supposed to give a hyperreal
number as the result.

It's clear that 8, 9, 10, \ldots is greater than 1, 1, 1, \ldots, because
every member of the first sequence is greater than every member of the second one.
But is 8, 9, 10, \ldots greater than 9, 9, 9, \ldots? We want the answer to be ``yes,'' because
we're thinking of the first one as an infinite number and the second one as the ordinary finite
number 9. The first sequence is indeed greater than the second at almost every one of the infinite number
of places at which they could be compared. The only place where it loses the contest is at the
very first position, and the only spot where we get a tie is the second one. Essentially the idea is
that we want to define a concept of what happens ``almost everywhere'' on some infinite list.
If one thing happens in an infinite number of places and something else only happens at
some finite number of spots, then the definition of ``almost everywhere'' is clear. What's harder
is a comparison of something like these two sequences:
\begin{equation*}
  2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, \ldots
\end{equation*}
and
\begin{equation*}
  1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, \ldots
\end{equation*}
where the second sequence has longer and longer runs of ones interspersed between the threes.
The two sequences are never equal at any position, so clearly they can't be considered to be
equal as hyperreal numbers. But there is an infinite number of spots in which the first
sequence is greater than the second, and likewise an infinite number in which it's less.
It seems as though there are more in which it's greater, so we probably want to define the
second sequence as being a hyperreal number that's less than 2. The problem is that it can
be very difficult to write down an acceptable definition of this ``almost everywhere''
notion. The answer is very technical, and I won't go into it here, but it can be done.
Because two sequences could be equal almost everywhere, we end up having to define a hyperreal number not
as a particular sequence but as a \emph{set} of sequences that are equal
to each other almost everywhere.

With the construction of this model, it is possible to prove that the hyperreals have the same level
of consistency as the reals.

\detour{transcendentals}{The transfer principle applied to functions}

On page \pageref{transcendentals}, I told you not to worry about whether it was legitimate to
apply familiar functions like $x^2$, $\sqrt{x}$, $\sin x$, $\cos x$, and $e^x$ to hyperreal numbers.
But since you're reading this, you're obviously in need of more reassurance.

For some of these functions, the transfer principle straightforwardly guarantees that they work
for hyperreals, have all the familiar properties, and can be computed in the same way. For example,
the following statement is in a suitable form to have the transfer principle applied to it:
\emph{
  For any real number $x$, $x\cdot x \ge0$.
}
Changing ``real'' to ``hyperreal,'' we find out that the square of a hyperreal number is
greater than or equal to zero, just like the square of a real number. Writing it as $x^2$ or
calling it a square is just a matter of notation and terminology.
The same applies to this statement:
\emph{
  For any real number $x\ge 0$, there exists a real number $y$ such that $y^2=x$.
}
Applying the transfer function to it tells us that square roots can be defined for
the hyperreals as well.

There's a problem, however, when we get to functions like $\sin x$ and $e^x$.
If you look up the definition of the sine function in a trigonometry textbook, it will
be defined geometrically, as the ratio of the lengths of two sides of a certain triangle.
The transfer principle doesn't apply to geometry, only to arithmetic. It's not even obvious
intuitively that it makes sense to define a sine function on the hyperreals. In an
application like the differentiation of the sine function on page \pageref{eg:derivative-of-sin},
we only had to take sines of hyperreal numbers that were infinitesimally close to real numbers,
but if the sine is going to be a full-fledged function defined on the hyperreals, then we should
be allowed, for example, to take the sine of an infinite number. What would that mean? If you
take the sine of a number like a million or a billion on your calculator, you just get some
apparently random result between $-1$ and 1. The sine function wiggles back and forth indefinitely
as $x$ gets bigger and bigger, never settling down to any specific limiting value. Apparently
we could have $\sin H=1$ for a particular infinite $H$, and then $\sin (H+\pi/2)=0$, $\sin(H+\pi)=-1$, \ldots

It turns out that the moral equivalent of the  transfer function can indeed be applied to any function on
the reals, yielding a function that is in some sense its natural ``big brother'' on the the hyperreals, but the consequences can be
either disturbing or exhilirating depending on your tastes.\index{transfer principle!applied to functions}
 For example, consider the function $[x]$ that takes
a real number $x$ and rounds it down to the greatest integer that is less than or equal to to $x$, e.g.,
$[3]=3$, and $[\pi]=3$. This function, like any other real function,
can be extended to the hyperreals, and that means that we can define
the \emph{hyperintegers},\index{hyperinteger}
the set of hyperreals that satisfy $[x]=x$. The hyperintegers include the integers as a subset,
but they also include infinite numbers. This is likely to seem magical, or even unreasonable, if we come
at the hyperreals from a purely axiomatic point of view.
The extension of functions to the hyperreals seems much more natural in view of the
construction of the hyperreals in terms of sequences given in the preceding section. For example,
the sequence $1.3, 2.3, 3.3, 4.3, 5.3, \ldots$ represents an infinite number. If we apply the $[x]$
function to it, we get $1, 2, 3, 4, 5, \ldots$, which is an infinite integer.

\detour{chain-rule}{Proof of the chain rule}

In the statement of the chain rule on page \pageref{sec:chain-rule}, I followed my usual custom of writing
derivatives as $\der y/\der x$, when actually the derivative is the standard part, $\st(\der y/\der x)$. In more rigorous
notation, the chain rule should be stated like this:
\begin{equation*}
  \st\left(\frac{\der z}{\der x}\right) =   \st\left(\frac{\der z}{\der y}\right) \st\left(\frac{\der y}{\der x}\right)\eqquad.
\end{equation*}
The transfer principle allows us to rewrite the left-hand side as $\st[(\der z/\der y)(\der y/\der x)]$, and then
we can get the desired result using the identity $\st(ab)=\st(a)\st(b)$.

\detour{exp}{Derivative of $e^x$}\index{derivative!of the exponential}

All of the reasoning on page \pageref{main:exp} would have applied equally well to any other
exponential function with a different base, such as $2^x$ or $10^x$. Those functions would have
different values of $c$, so if we want to determine the value of $c$ for the base-$e$ case, we
need to bring in the definition of $e$, or of the exponential function $e^x$, somehow.

We can take the definition of $e^x$ to be\label{definition-of-exp}\index{exponential!definition of}
\begin{align*}
  e^x = \lim_{n\rightarrow \infty} \left(1+\frac{x}{n}\right)^n\eqquad.
\end{align*}
The idea behind this relation is similar to the idea of compound interest. If the interest rate is 10\%, compounded
annually, then $x=0.1$, and the balance grows by a factor $(1+x)=1.1$ in one year. If, instead, we want to compound the
interest monthly, we can set the monthly interest rate to $0.1/12$, and then the growth of the
balance over a year is $(1+x/12)^{12}=1.1047$, which is slightly larger because the interest from the earlier months
itself accrues interest in the later months. Continuing this limiting process, we find $e^{1.1}=1.1052$.

If $n$ is large, then we have a good approximation to the base-$e$ exponential, so let's differentiate
this finite-$n$ approximation and try to find an approximation to the derivative of $e^x$. The chain rule
tells is that the derivative of $(1+x/n)^n$ is the derivative of the raising-to-the-nth-power function,
multiplied by the derivative of the inside stuff, $\der(1+x/n)/\der x=1/n$. We then have
\begin{align*}
  \frac{\der \left(1+\frac{x}{n}\right)^n}{\der x} &= \left[n\left(1+\frac{x}{n}\right)^{n-1}\right]\cdot \frac{1}{n} \\
            &= \left(1+\frac{x}{n}\right)^{n-1}\eqquad.
\end{align*}
But evaluating this at $x=0$ simply gives 1, so at $x=0$, the approximation to the derivative is exactly 1 for all values of
$n$ --- it's not even necessary to imagine going to larger and larger values of $n$. This establishes that $c=1$,
so we have
\begin{equation*}
  \frac{\der e^x}{\der x} = e^x 
\end{equation*}
for all values of $x$.

\detour{lhospital-proofs}{Proofs of the generalizations of l'H\^{o}pital's rule}\index{l'H\^{o}pital's rule!general form!proofs}
\emph{Multiple applications of the rule}

Here we prove, as claimed on p.~\pageref{lhospital-multiple}, that the form of L'H\^{o}pital's rule rule given
on p.~\pageref{lhospital-simple} can be generalized to the case where more than one application of the
rule is required. The proof requires material from ch.~\ref{ch:integration} (integration and the mean value theorem),
and, as discussed in example \ref{eg:taylor-lhospital} on p.~\pageref{eg:taylor-lhospital}, 
the motivation for the result becomes much more transparent once has read ch.~\ref{ch:sequences} and knows
about Taylor series. The reader who has arrived here while reading ch.~\ref{ch:limits} will need to defer reading
this section of the proof until after ch.~\ref{ch:integration}, and may wish to wait until after ch.~\ref{ch:sequences}.

The proof can be broken down into two steps.

Step 1: We first have to establish a stronger form of l'H\^{o}pital's rule
that states that $\lim u/v=\lim \dot{u}/\dot{v}$ rather than $\lim u/v=\dot{u}/\dot{v}$.
This form is stronger, because in a case like example \ref{eg:lhospital-differentiating-twice} on p.~\pageref{eg:lhospital-differentiating-twice},
$\dot{u}/\dot{v}$ isn't defined, but $\lim \dot{u}/\dot{v}$ is.

We prove the stronger form using the mean value theorem (p.~\pageref{mean-value-theorem-stated}).
For simplicity of notation, let's assume that the limit is being taken at $x=0$. By the fundamental theorem
of calculus, we have $u(x)=\int_0^x \dot{u}(x')\der x'$, and the mean value theorem then tells us that
for some $p$ between 0 and $x$, $u(x)=x\dot{u}(p)$. Likewise for a $q$ in this interval, $v(x)=x\dot{v}(q)$. 
So
\begin{equation*}
  \lim_{x\rightarrow 0} \frac{u}{v} = \lim_{\substack{p\rightarrow 0 \\q\rightarrow 0}} \frac{\dot{u}(p)}{\dot{v}(q)}\eqquad,
\end{equation*}
but since both $p$ and $q$ are closer to zero than $x$ is, the limit as they simultaneously approach zero is
the same as the limit as $x$ approaches zero.

Step 2: If we need
to take $n$ derivatives, the proof follows by applying the extra-strength rule $n$ times.\footnote{There is a logical
subtlety here, which is that although we've given a clearcut recipe for cooking up a proof for any given
$n$, that isn't quite the same thing as proving it for any positive integer $n$. This is an example where what we really need is a technique called
proof by induction. In general, proof by induction works like this. Suppose we prove some statement
about the integer 1, e.g., that l'H\^{o}pital's rule is valid when you take 1 derivative. Now say
that we can also prove that if that statement holds for a given $n$, it also holds for $n+1$. Proof by
induction means that we can then consider the statement as having been proved for all positive integers.
For suppose the contrary. Then there would be some least $n$ for which it failed, but this would
be a contradiction, since it would hold for $n-1$.}

\emph{Change of variable}

We will build up the rest of the features of l'H\^{o}pital's rule  using the technique of a change of variable. To demonstrate
how this works, let's imagine that we were starting from an even more stripped-down version of l'H\^{o}pital's rule
than the one on p.~\pageref{lhospital-simple}. Say we only knew how to do limits of the form $x\rightarrow0$ rather than
$x\rightarrow a$ for an arbitrary real number $a$. We could then evaluate $\lim_{x\rightarrow a} u/v$ simply
by defining $t=x-a$ and reexpressing $u$ and $v$ in terms of $t$.

\begin{eg}
\egquestion
Reduce
\begin{equation*}
  \lim_{x\rightarrow \pi}\frac{\sin x}{x-\pi}
\end{equation*}
to a form involving a limit at 0.

\eganswer
Define $t=x-\pi$. Solving for $x$ gives $x=t+\pi$. We substitute into the above expression
to find
\begin{equation*}
  \lim_{x\rightarrow \pi}\frac{\sin x}{x-\pi} =   \lim_{t\rightarrow 0}\frac{\sin (t+\pi)}{t}\eqquad.
\end{equation*}
If all we knew was the $\rightarrow0$ form of l'H\^{o}pital's rule, then this would suffice
to reduce the problem to one we knew how to solve. In fact, this kind of change of variable works in
all cases, not just for a limit at $\pi$, so rather then going through a laborious change of variable
every time, we could simply establish the more general form on p.~\pageref{lhospital-simple}, with
$\rightarrow a$.
\end{eg}

\emph{The indeterminate form $\infty/\infty$}\label{lhospital-inf-inf}

To prove that l'H\^{o}pital's rule works in general for $\infty/\infty$ forms, 
we do a change of variable on the \emph{outputs} of the functions $u$ and $v$
rather than their inputs. Suppose that our original problem is of the form
\begin{equation*}
  \lim \frac{u}{v}\eqquad,
\end{equation*}
where both functions blow up.\footnote{Think about what happens when only $u$ blows up,
or only $v$.} We then define $U=1/u$ and $V=1/v$. We now have
\begin{equation*}
  \lim \frac{u}{v} = \lim \frac{1/U}{1/V} =\lim \frac{V}{U}\eqquad,
\end{equation*}
and since $U$ and $V$ both approach zero, we have reduced the problem to one that can be solved
using the version of l'H\^{o}pital's rule already proved
for indeterminate forms like $0/0$. Differentiating and applying the chain rule, we have
\begin{equation*}
  \lim \frac{u}{v} = \lim \frac{\dot{V}}{\dot{U}} = \lim \frac{-v^{-2}\dot{v}}{-u^{-2}\dot{u}}\eqquad.
\end{equation*}
Since $\lim ab=\lim a \lim b$ provided that $\lim a$ and $\lim b$ are both defined, we can rearrange
factors to produce the desired result.



This change of variable is a specific example of a much more general method of problem-solving in which we look for a way to reduce a hard problem
to an easier one. We will encounter changes of variable again on p.~\pageref{change-of-variable-for-integration} as a technique for
integration, which means undoing the operation of differentiation.

\detour{fundamental-thm-proof}{Proof of the fundamental theorem of calculus}\index{calculus!fundamental theorem of!proof}\index{fundamental theorem of calculus!proof}

There are three parts to the proof: (1)  Take the equation that states
the fundamental theorem, differentiate both sides with respect to $b$, and show that they're equal.
(2) Show that continuous functions with equal derivatives must be essentially the same function, except for an
additive constant. (3) Show that the constant in question is zero.

1. By the definition of the indefinite integral, the derivative of $x(b)-x(a)$ with respect to $b$ equals
$\xdot(b)$. We have to establish that this equals the following:
\begin{align*}
  \frac{\der}{\der b} \int_a^b \xdot(t) \der t 
    &= \st \frac{1}{\der b}\left[\int_a^{b+\der b}  \xdot(t) \der t - \int_a^b  \xdot(t) \der t\right] \\
    &= \st \frac{1}{\der b}\int_b^{b+\der b}  \xdot(t) \der t \\
    &= \st \frac{1}{\der b}\lim_{H\rightarrow\infty} \sum_{i=0}^H  \xdot(b\:+\:i\der b/H) \frac{\der b}{H} \\
    &= \st \lim_{H\rightarrow\infty} \frac{1}{H} \sum_{i=0}^H  \xdot(b\:+\:i\der b/H)
\end{align*}
Since $\xdot$ is continuous, all the values of $\xdot$ occurring inside the sum can differ
only infinitesimally from $\xdot(b)$. Therefore the quantity inside the limit differs only infinitesimally from
$\xdot(b)$, and the standard part of its limit must be $\xdot(b)$.\footnote{If you don't want to use infinitesimals,
then you can express the derivative as a limit, and in the final step of the argument use the mean value theorem,
introduced later in the chapter.}

2. Suppose $f$ and $g$ are two continuous functions whose derivatives are equal. Then $d=f-g$ is a continuous function whose derivative is zero.
But the only continuous function with a derivative of zero is a constant, so $f$ and $g$ differ by at most an additive constant.

3. I've established that the derivatives with respect to $b$ of $x(b)-x(a)$ and $\int_a^b \xdot \der t$ are the same, so they differ by
at most an additive constant. But at $b=a$, they're both zero, so the constant must be zero.

\pagebreak

\detour{intermediate-value}{The intermediate value theorem}\index{intermediate value theorem}

On page \pageref{intermediate-value-ref-to-detour} I asserted that the intermediate value theorem was
really more a statement about the (real or hyperreal) number system than about functions. For insight,
consider figure \figref{euclid}, which is a geometrical construction that constitutes the proof of the
very first proposition in Euclid's celebrated \emph{Elements}. The proposition to be proved is that given
a line segment AB, it is possible to construct an equilateral triangle with AB as its base. The proof is
by construction; that is, Euclid doesn't just give a logical argument that convinces us the triangle must
exist, he actually demonstrates how to construct it. First we draw a circle with center
A and radius AB, which his third postulate says we can do. Then we draw another circle with the same radius,
but centered at B. Pick one of the intersections of the circles and call it C. Construct the line segments
AC and BC (postulate 1). Then AC equals AB by the definition of the
circle, and likewise BC equals AB. Euclid also has an axiom that things equal
to the same thing are equal to one another, so it follows that AC equals BC, and therefore the triangle is equilateral.

\fig{euclid}{A proof from Euclid's \emph{Elements}.}

It seems like a model of mathematical rigor, but there's a flaw in the reasoning, which is that he assumes
without justififcation that the circles do have a point in common. To see that this is not as secure an assumption as it seems, consider
the usual Cartesian representation of plane geometry in terms of coordinates $(x,y)$.
Usually we assume that $x$ and $y$ are real numbers. What if we instead do our Cartesian geometry
using rational numbers as coordinates? Euclid's five postulates are all consistent with this.
For example, circles do exist. Let $\zu{A}=(0,0)$ and $\zu{B}=(1,0)$. Then there are infinitely
many pairs of rational numbers in the set that satisfies the definition of the circle centered at A. Examples include $(3/5,4/5)$
and $(-7/25,24/25)$. The circle is also continuous in the sense that if I specify a point on it such
as $(-7/25,24/25)$, and a distance that I'm allowed to make as small as I please, say $10^{-6}$, then other
points exist on the circle within that distance of the given point. However, the intersection
assumed by Euclid's proof doesn't exist. It would lie at $(1/2,\sqrt{3}/2)$, but $\sqrt{3}$ doesn't
exist in the rational number system.

In exactly the same way, we can construct counterexamples to the intermediate value theorem if
the underlying system of numbers doesn't have the same properties as the real numbers. For example,
let $y=x^2$. Then $y$ is a continuous function, on the interval from 0 to 1, but if we take the rational
numbers as our foundation, then there is no $x$ for which $y=1/2$. The solution would be $x=1/\sqrt{2}$, which
doesn't exist in the rational number system. Notice the similarity between this problem and the one in
Euclid's proof. In both cases we have curves that cut one another without having an intersection.
In the present example, the curves are the graphs of the functions $y=x^2$ and $y=1/2$.

The interpretation is that the real numbers are in some sense more densely packed than the rationals,
and with two thousand years worth of hindsight, we can see that Euclid should have included a
sixth postulate that expressed this density property. One possible way of stating such a postulate
is the following. Let L be a ray, and O its endpoint. We think of O as the
origin of the positive number line.
Let P and Q be sets of points on L such that every point in P is closer to O than every point in Q.
Then there exists some point Z on L such that Z lies at least as far from O as every point in P,
but no farther than any point in Q. Technically this property is known as \emph{completeness}.\index{completeness}\label{compactness}
As an example, let $\zu{P}=\{x|x^2<2\}$ and $\zu{Q}=\{x|x^2 \ge 2\}$. Then the point Z would have to be $\sqrt{2}$,
which shows that the rationals are not complete. The reals are complete, and the completeness
axiom can serve as one of the fundamental axioms of the real numbers.

Note that the axiom
refers to \emph{sets} P and Q, and says that a certain fact is true for any choice of those sets;
it therefore isn't the type of proposition that is covered by the transfer principle, and in fact
it fails for the hyperreals, as we can see if P is the set of all infinitesimals and Q the
positive real numbers.

Here is a skeletal proof of the intermediate value theorem, in which I'll make some simplifying
assumptions and leave out some cases. We want to prove that if $y$ is a continuous real-valued function on the real interval from $a$ to $b$,
and if $y$ takes on values $y_1$ and $y_2$ at certain points within this interval, then for any $y_3$ between $y_1$ and
$y_2$, there is some real $x$ in the interval for which $y(x)=y_3$.
I'll assume the case in which $x_1<x_2$ and $y_1<y_2$.
Define sets of real numbers $\zu{P}=\{x|y \le y_3\}$, and let $\zu{Q}=\{x|y \ge y_3\}$.
For simplicity, I'll assume that every member of P is less than or equal to
every member of Q, which happens, for example, if the function $y(x)$ is always increasing
on the interval $[a,b]$. If P and Q intersect, then the theorem holds.
Suppose instead that P and Q do not intersect.
Using the completeness axiom, there exists some real $x$ which is greater than or equal
to every element of P and less than or equal to every element of Q.
Suppose $x$ belongs to P.
Then the following
statement is in the right form for the transfer principle to apply to it: for any number
$x'>x$, $y(x')>y_3$. We can conclude that the statement is also true for the hyperreals,
so that if $\der x$ is a positive infinitesimal and $x'=x+\der x$, we have $y(x)<y_3$, but $y(x+\der x)>y_3$.
Then by continuity, $y(x)-y(x+\der x)$ is infinitesimal. But $y(x)<y_3$ and
$y(x+\der x)>y_3$, so the standard part of $y(x)$ must equal $y_3$. By assumption $y$ takes on real values
for real arguments, so $y(x)=y_3$. The same reasoning applies if $x$ belongs to Q, and since
$x$ must belong either to P or to Q, the result is proved.

For an alternative proof of the
intermediate value theorem by an entirely different technique, see Keisler (references, p. \pageref{references}).

As a side issue, we could ask whether there is anything like the intermediate value theorem
that can be applied to functions on the hyperreals. Our definition of continuity on page \pageref{def-continuity}
explicitly states that it only applies to real functions.
Even if we could apply the definition to a function on the hyperreals, the proof given above would fail, since the hyperreals lack the completeness
property. As a counterexample, let $\epsilon$ be some positive infinitesimal, and define
a function $y$ such that $y=-\epsilon$ when $\zu{st}(x) \le 0$ and $y=\epsilon$ everywhere else.
If we insist on applying the definition of continuity to this function, it appears to be continuous,
so it violates the intermediate value theorem. Note, however, that the way this function is defined
is different from the way we usually define functions on the hyperreals. Usually we define a function
on the reals, say $y=x^2$, in language to which the transfer principle applies, and then we use the
transfer principle to reason about the function's analog on the hyperreals. For instance, the function
$y=x^2$ has the property that $y \ge 0$ everywhere, and the transfer principle guarantees that that's
also true if we take $y=x^2$ as the definition of a function on the hyperreals. For functions defined
in this way, the intermediate value theorem makes a statement that the transfer principle applies
to, and it is therefore true for the hyperreal version of the function as well.

\detour{extreme-value}{Proof of the extreme value theorem}\index{extreme value theorem!proof}

The extreme value theorem was stated on page \pageref{extreme-value-theorem}. Before we can prove
it, we need to establish some preliminaries, which turn out to be interesting for their own sake.

Definition: Let $C$ be a subset of the real numbers whose definition can be expressed
in the type of language to which the transfer principle applies. Then $C$ is \emph{compact}\index{compact set}
if for every hyperreal number $x$ satisfying the definition of $C$, the standard part of $x$ exists
and is a member of $C$.

To understand the content of this definition, we need to look at the two ways in which a set could
fail to satisfy it. 

First, suppose $U$ is defined by $x \ge 0$. Then there are positive infinite
hyperreal numbers that satisfy the definition, and their standard part is not defined, so $U$ is not
compact. The reason $U$ is not compact is that it is unbounded.

Second, let $V$ be defined by $0 \le x < 1$. Then if $dx$ is a positive infinitesimal,
$1-dx$ satisfies the definition of $V$, but its standard part is 1, which is not in $V$, so $V$
is not compact. The set $V$ has boundary points at 0 and 1, and the reason it is not compact
is that it doesn't contain its right-hand boundary point. A boundary point\index{boundary point}
is a real number which is infinitesimally close to some points inside the set, and also to some
other points that are on the outside.

We therefore arrive at the following alternative characterization of the notion of a compact
set, whose proof is straightforward.

Theorem: A set is compact if and only if it is bounded and contains all of its boundary points.

Intuitively, the reason compact sets are interesting is that if you're standing inside a compact
set and start taking steps in a certain direction, without ever turning around, you're guaranteed to
approach some point in the set as a limit. (You might step over some gaps that aren't included in the set.)
If the set was unbounded, you could just walk forever at a constant speed.
If the set didn't contain its boundary point, then you could asymptotically approach the boundary, but
the goal you were approaching wouldn't be a member of the set.

The following theorem turns out to be the most difficult part of the discussion.

Theorem: A compact set contains its maximum and minimum.\\
Proof: Let $C$ be a compact set. We know it's bounded, so let $M$ be the set of all real numbers
that are greater than any member of $C$. By the completeness property of the real
numbers, there is some real number $x$ between $C$ and $M$. Let $^{*}C$ be the set of hyperreal numbers that satisfies the same
definition that $C$ does.

Every real $x'$ greater than $x$ fails
to satisfy the condition that defines $C$, and by the transfer principle the same must be true if $x'$ is any hyperreal,
so if $dx$ is a positive infinitesimal, $x+dx$ must be outside of $^{*}C$. 

But now consider $x-dx$.
The following statement holds for the reals: there is no number $x'<x$ that is greater than every
member of $C$. By the transfer principle, we find that there is some hyperreal number $q$ in $^{*}C$
that is greater than $x-dx$. But the standard part of $q$ must equal $x$, for otherwise $\zu{st} q$ would
be a member of $C$ that was greater than $x$.
Therefore $x$ is a boundary point of $C$, and since
$C$ is compact, $x$ is a member of $C$. We conclude $C$ contains its maximum. A similar argument shows that
$C$ contains its minimum, so the theorem is proved.

There were two subtle things about this proof. The first was that we ended up constructing the
set of hyperreals $^{*}C$, which was the hyperreal ``big brother'' of the real set $C$. This
is exactly the sort of thing that the transfer principle does \emph{not} guarantee we can do.
However, if you look back through the proof, you can see that $^{*}C$ is used only as a notational
convenience. Rather than talking about whether a certain number was a member of $^{*}C$, we could
have referred, more cumbersomely, to whether or not it satisfied the condition that had originally
been used to define $C$. The price we paid for this was a slight loss of generality. There are so
many different sets of real numbers that they can't possibly all have explicit definitions that can
be written down on a piece of paper. However, there is very little reason to be interested in
studying the properties of a set that we were never able to define in the first place. The
other subtlety was that we had to construct the auxiliary
point $x-dx$, but there was not much we could actually say about $x-dx$ itself. In particular, it
might or might not have been a member of $C$. For example, if $C$ is defined by the condition $x=0$, then
$^{*}C$ likewise contains only the single element 0, and $x-dx$ is not a member of $^{*}C$.
But if $C$ is defined by $0 \le x \le 1$, then $x-dx$ is a member of $^{*}C$.

The original goal was to prove the extreme value theorem, which is a statement about continuous functions, but so far we haven't said anything about functions.

Lemma: Let $f$ be a real function defined on a set of points $C$. Let $D$ be the image of $C$, i.e.,
the set of all values $f(x)$ that occur for some $x$ in $C$. Then if $f$ is continous and $C$ is
compact, $D$ is compact as well. In other words, continuous functions take compact sets to compact sets.\\
Proof: Let $y=f(x)$ be any hyperreal output corresponding to a hyperreal input $x$ in $^{*}C$.
We need to prove that the standard part of $y$ exists, and is a member of $D$. Since $C$ is compact,
the standard part of $x$ exists and is a member of $C$. But then by continuity $y$ differs only
infinitesimally from $f(\zu{st} x)$, which is real, so $\zu{st} y=f(\zu{st} x)$ is defined and is a member
of $D$.

We are now ready to prove the extreme value theorem, in a version slightly more general than the one
originally given on page \pageref{extreme-value-theorem}.

The extreme value theorem: Any continuous function on a compact set achieves a maximum and minimum value,
and does so at specific points in the set.

Proof: Let $f$ be continuous, and let $C$ be the compact set on which we seek its maximum and minimum.
Then the image $D$ as defined in the lemma above is compact. Therefore $D$ contains its maximum and
minimum values.

\detour{mean-value-proof}{Proof of the mean value theorem}\index{mean value theorem!proof}

Suppose that the mean value theorem is violated. Let $L$ be the set of all $x$ in the interval from $a$ to $b$
such that $y(x)<\bar{y}$, and likewise let $M$ be the set with $y(x)>\bar{y}$. If the theorem is violated, then
the union of these two sets covers the entire interval from $a$ to $b$. Neither one can be empty; if, for example, $M$ was
empty, then we would have $y<\bar{y}$ everywhere and also $\int_a^b y=\int_a^b\bar{y}$, but it follows directly from
the definition of the definite integral that when one function is less than another, its integral is also less
than the other's. Since $y$ takes on values less than and greater than $\bar{y}$, it follows from the intermediate value theorem
that $y$ takes on the value $\bar{y}$ somewhere (intuitively, at a boundary between $L$ and $M$).

\pagebreak

\detour{fn-thm-alg-proof}{Proof of the fundamental theorem of algebra}\index{fundamental theorem of algebra!proof}

We start with the following lemma, which is intuitively obvious, because polynomials don't have asymptotes.
Its proof is given after the proof of the main theorem.

Lemma: For any polynomial $P(z)$ in the complex plane, its magnitude $|P(z)|$ achieves its minimum
value at some specific point $z_\zu{o}$.

The fundamental theorem of algebra: In the complex number system, a nonzero nth-order polynomial has exactly $n$
roots, i.e., it can be factored into the form $P(z)=(z-a_1)(z-a_2)\ldots(z-a_n)$,
where the $a_i$ are complex numbers.

Proof: The proofs in the cases of $n=0$ and 1 are trivial, so our strategy is to
reduce higher-$n$ cases to lower ones. If an nth-degree polynomial $P$ has at least
one root, $a$, then we can always reduce it to a polynomial of degree $n-1$ by
dividing it by $(z-a)$. Therefore the theorem is proved by induction provided that
we can show that every polynomial of degree greater than zero has at least one
root.

Suppose, on the contrary, that there is an nth order polynomial $P(z)$, with $n>0$, that has
no roots at all. Then by the lemma $|P|$ achieves its minimum value
at some point $z_\zu{o}$. To make things more
simple and concrete, we can construct another polynomial $Q(z)=P(z+z_\zu{o})/P(z_\zu{o})$,
so that $|Q|$ has a minimum value of 1, achieved at $Q(0)=1$. This means that
$Q$'s constant term is 1. What about its other terms? Let $Q(z)=1+c_1z+\ldots+c_nz^n$.
Suppose $c_1$ was nonzero.
Then for infinitesimally small values of $z$, the terms of order $z^2$ and higher
would be negligible, and we could make $Q(z)$ be a real number less than one by
an appropriate choice of $z$'s argument. Therefore $c_1$ must be zero. But that means
that if $c_2$ is nonzero, then for infinitesimally small $z$, the $z^2$ term dominates
the $z^3$ and higher terms, and again this would allow us to make $Q(z)$ be real and
less than one for appropriately chosen values of $z$. Continuing this process, we
find that $Q(z)$ has no terms at all beyond the constant term, i.e., $Q(z)=1$. This
contradicts the assumption that $n$ was greater than zero, so we've proved by
contradiction that there is no $P$ with the properties claimed.

Uninteresting proof of the lemma: Let $M(r)$ be the minimum value of $|P(z)|$ on the disk defined by $|z| \le r$.
We first prove that $M(r)$ can't asymptotically approach a minimum as $r$ approaches infinity.
Suppose to the contrary: for every $r$, there is some $r'>r$ with $M(r')<M(r)$.
Then by the transfer principle, the same would have to be true for
hyperreal values of $r$. But it's clear that if $r$ is infinite, the lower-order terms of $P$
will be infinitesimally small compared to the highest-order term, and therefore $M(r)$ is
infinite for infinite values of $r$, which is a contradiction, since by construction $M$ is
decreasing, and finite for finite $r$. We can therefore conclude
by the extreme value theorem that $M$ achieves its minimum for some specific value of $r$.
The least such $r$ describes a circle $|z|=r$ in the complex plane, and the minimum of $|P|$
on this circle must be the same as its global minimum. Applying the extreme value function
to $|P(z)|$ as a function of arg $z$ on the interval $0 \le \zu{arg} z \le 2\pi$, we establish
the desired result.
