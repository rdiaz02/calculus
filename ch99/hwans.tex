%%chapter%% 99
\chapter{Answers and solutions}

\hwanssection{Answers to Self-Checks}

\beginscanswers{4}

\scanshdr{interpret-human-height}

The area under the curve from 130 to 135 cm is about 3/4 of a rectangle. The area from
135 to 140 cm is about 1.5 rectangles. The number of people in the second range is
about twice as much. We could have converted these to actual probabilities
($1\ \text{rectangle}=5\ \zu{cm}\times0.005\ \zu{cm}^{-1}=0.025$), but that would
have been pointless, because we were just going to compare the two areas.

\beginscanswers{6}

\scanshdr{complex-square-root}
Say we're looking for $u=\sqrt{z}$, i.e., we want a number $u$ that, multiplied by
itself, equals $z$.
Multiplication multiplies the magnitudes, so
the magnitude of $u$ can be found by taking the square root of the magnitude of $z$.
Since multiplication also adds the arguments of the numbers, squaring a number
doubles its argument. Therefore
we can simply divide the argument of $z$ by two to find the argument of
$u$. This results in one of the square roots of $z$. There is another one, which
is $-u$, since $(-u)^2$ is the same as $u^2$.
This may seem a little odd: if $u$ was chosen so that doubling its
argument gave the argument of $z$, then how can the same be true for $-u$?
Well for example, suppose the argument of $z$ is $4\degunit$. Then $\arg u=2\degunit$,
and $\arg(-u)=182\degunit$. Doubling 182 gives 364, which is actually a synonym for
4 degrees.

\vfill\pagebreak[4]

\hwanssection{Solutions to homework problems}

\beginsolutions{1}

\hwsolnhdr{check-twot-graph}

The tangent line has to pass through the point (3,9), and it also seems, at least approximately, to pass
through (1.5,0). This gives it a slope of $(9-0)/(3-1.5)=9/1.5=6$, and that's exactly what $2t$ is at $t=3$.
%%graph%% soln-check-twot-graph func=x**2 format=eps xlo=0 xhi=4 ylo=0 yhi=16 x=t y=x ytic_spacing=2 more_space_below=6 more_space_above=10 ; func=6*x-9
\smallfig{soln-check-twot-graph}{Problem \ref{hw:check-twot-graph}.}

\hwsolnhdr{graph-sin-et}

The tangent line has to pass through the point $(0,\sin(e^0))=(0,0.84)$, and it also seems, at least approximately, to pass
through (-1.6,0). This gives it a slope of $(0.84-0)/(0-(-1.6))=0.84/1.6=0.53$. The more accurate result given in the problem
can be found using the methods of chapter 2.

%%graph%% soln-graph-sin-et func=sin(exp(x)) format=eps xlo=-2 xhi=2 ylo=-1 yhi=1.5 x=t y=x ytic_spacing=1 more_space_below=6 more_space_above=10 ; func=.540302*x+sin(1)
\smallfig{soln-graph-sin-et}{Problem \ref{hw:graph-sin-et}.}

\pagebreak[4]

\hwsolnhdr{diff-monomials}\\*
The derivative is a rate of change, so the derivatives of the constants 1 and 7, which don't change, are
clearly zero. The derivative can be interpreted geometrically as the slope of the tangent line, and since
the functions $t$ and $7t$ \emph{are} lines, their derivatives are simply their slopes, 1, and 7. 
All of these could also have been found using the formula that says the derivative of $t^k$ is
$kt^{k-1}$, but it wasn't really necessary to get that fancy.
To find the derivative of $t^2$, we can use the formula, which gives $2t$. One of the properties of
the derivative is that multiplying a function by a constant multiplies its derivative by the same
constant, so the derivative of $7t^2$ must be $(7)(2t)=14t$. By similar reasoning, the derivatives
of $t^3$ and $7t^3$ are $3t^2$ and $21t^2$, respectively.

\hwsolnhdr{diff-polynomial}

One of the properties of the derivative is that the derivative of a sum is the sum of the derivatives,
so we can get this by adding up the derivatives of $3t^7$, $-4t^2$, and 6. The derivatives of the
three terms are $21t^6$, $-8t$, and 0, so the derivative of the whole thing is $21t^6-8t$.

\hwsolnhdr{diff-symbolic-const}

This is exactly like problem \ref{hw:diff-polynomial}, except that instead of explicit
numerical constants like 3 and $-4$, this problem involves symbolic constants $a$, $b$, and
$c$. The result is $2at+b$.

\hwsolnhdr{same-deriv}

The first thing that comes to mind is $3t$. Its graph would be a line with a slope of 3,
passing through the origin. Any other line with a slope of 3 would work too, e.g.,
$3t+1$.

\hwsolnhdr{integ-monomial}

Differentiation lowers the power of a monomial by one, so to get something with an exponent
of 7, we need to differentiate something with an exponent of 8. The derivative of
$t^8$ would be $8t^7$, which is eight times too big, so we really need $(t^8/8)$.
As in problem \ref{hw:same-deriv}, any other function that differed by an additive constant
would also work, e.g., $(t^8/8)+1$.

\hwsolnhdr{integ-monomial-const}

This is just like problem \ref{hw:integ-monomial}, but we need something whose derivative
is three times bigger. Since multiplying by a constant multiplies the derivative by the
same constant, the way to accomplish this is to take the answer to problem \ref{hw:integ-monomial},
and multiply by three. A possible answer is $(3/8)t^8$, or that function plus any constant.

\hwsolnhdr{integ-polynomial}

This is just a slight generalization of problem \ref{hw:integ-monomial-const}. Since
the derivative of a sum is the sum of the derivatives, we just need to handle each term
individually, and then add up the results. The answer is
$(3/8)t^8-(4/3)t^3+6t$, or that function plus any constant.

\hwsolnhdr{observable-universe}

The function $v=(4/3)\pi(ct)^3$ looks scary and complicated, but it's nothing more than a constant
multiplied by $t^3$, if we rewrite it as $v=\left[(4/3)\pi c^3\right]t^3$. The whole thing
in square brackets is simply one big constant, which just comes along for the ride
when we differentiate. The result is $\dot{v}=\left[(4/3)\pi c^3\right](3t^2)$, or,
simplifying, $\dot{v}=\left(4\pi c^3\right)t^2$. (For further physical insight, we can
factor this as $\left[4\pi (ct)^2\right]c$, where $ct$ is the radius of the expanding sphere, and
the part in brackets is the sphere's surface area.)

For purposes of checking the units, we can ignore the unitless constant $4\pi$, which just
leaves $c^3t^2$. This has units of $(\text{meters per second})^3(\text{seconds})^2$, which
works out to be cubic meters per second. That makes sense, because it tells us how quickly
a volume is increasing over time.

\hwsolnhdr{ke}\\*
This is similar to problem \ref{hw:observable-universe}, in that it looks scary, but we can rewrite
it as a simple monomial, $K=(1/2)mv^2=(1/2)m(at)^2=(ma^2/2)t^2$. The derivative is
$(ma^2/2)(2t)=ma^2t$. The car needs more and more power to accelerate as its speed increases.

To check the units, we just need to show that the expression $ma^2t$ has units that are like
those of the original expression for $K$, but divided by seconds, since it's a rate of
change of $K$ over time. This indeed works out, since the only change in the factors that
aren't unitless is the reduction of the powet of $t$ from 2 to 1.

\hwsolnhdr{expanding-square}

The area is $a=\ell^2=(1+\alpha T)^2\ell_\zu{o}^2$. To make this into something we know how
to differentiate, we need to square out the expression involving $T$, and make it into something
that is expressed explicitly as a polynomial:
\begin{equation*}
  a=\ell_\zu{o}^2+2\ell_\zu{o}^2\alpha T+\ell_\zu{o}^2\alpha^2T^2
\end{equation*}
Now this is just like problem \ref{hw:diff-symbolic-const}, except that the constants superficially
look more complicated. The result is
\begin{align*}
  \dot{a} &=2\ell_\zu{o}^2\alpha +2\ell_\zu{o}^2\alpha^2T \\
          &=2\ell_\zu{o}^2\left(\alpha +\alpha^2T\right)\eqquad.
\end{align*}

We expect the units of the result to be area per unit temperature, e.g., degrees per square meter.
This is a little tricky, because we have to figure out what units are implied for the constant
$\alpha$. Since the question talks about $1+\alpha T$, apparently the quantity $\alpha T$ is unitless.
(The 1 is unitless, and you can't add things that have different units.) Therefore the units
of $\alpha$ must be ``per degree,'' or inverse degrees. It wouldn't make sense to add $\alpha$ and
$\alpha^2T$ unless they had the same units (and you can check for yourself that they do), so
the whole thing inside the parentheses must have units of inverse degrees. Multiplying by the
$\ell_\zu{o}^2$ in front, we have units of area per degree, which is what we expected.

\hwsolnhdr{second-deriv}

The first derivative is $6t^2-1$. Going again, the answer is $12t$.

\hwsolnhdr{inflection}

The first derivative is $3t^2+2t$, and the second is $6t+2$. Setting this equal to zero
and solving for $t$, we find $t=-1/3$. Looking at the graph, it does look like the concavity
is down for $t<-1/3$, and up for $t>-1/3$.
%
%%graph%% soln-inflection func=x**3+x**2 format=eps xlo=-1 xhi=.5 ylo=0 yhi=0.5 x=t y=x ytic_spacing=.5 more_space_below=6 more_space_above=10
\smallfig{soln-inflection}{Problem \ref{hw:inflection}.}

\hwsolnhdr{neg-power-graph}

I chose $k=-1$, and $t=1$. In other words, I'm going to check the slope of the function $x=t^{-1}=1/r$ at $t=1$, and
see whether it really equals $kt^{k-1}=-1$. Before even doing the graph, I note that the sign makes sense: the function
$1/t$ is decreasing for $t>0$, so its slope should indeed be negative.

%%graph%% soln-neg-power-graph func=1/x format=eps xlo=0 xhi=3 ylo=0 yhi=3 x=t y=x ytic_spacing=1 more_space_below=6 more_space_above=10 ; func=-x+2
\smallfig{soln-neg-power-graph}{Problem \ref{hw:neg-power-graph}.}

The tangent line seems to connect the points (0,2) and (2,0), so its slope does indeed look like it's $-1$.

The problem asked us to consider the logical meaning of the two possible outcomes. If the slope had been significantly
different from $-1$ given the accuracy of our result, the conclusion would have been that it was incorrect to extend the
rule to negative values of $k$. Although our example did come out consistent with the rule, that doesn't prove the rule
in general. An example can disprove a conjecture, but can't prove it. Of course, if we tried lots and lots of examples,
and they all worked, our confidence in the conjecture would be increased.

\hwsolnhdr{lennard-jones}

A minimum would occur where the derivative was zero. First we rewrite the function in a form that we
know how to differentiate:
\begin{equation*}
  E(r) = ka^{12}r^{-12}-2ka^6r^{-6}
\end{equation*}
We're told to have faith that the derivative of $t^k$ is $kt^{k-1}$ even for $k<0$, so
\begin{align*}
  0 &= \dot{E} \\
    &= -12ka^{12}r^{-13}+12ka^6r^{-7} 
\end{align*}
To simplify, we divide both sides by $12k$. The left side was already zero, so it keeps being zero.
\begin{gather*}
  0  = -a^{12}r^{-13}+a^6r^{-7} \\
  a^{12}r^{-13} = a^6r^{-7} \\
  a^{12} = a^6r^6 \\
  a^6 = r^6 \\
  r = \pm a
\end{gather*}

To check that this is a minimum, not a maximum or a point of inflection, one method is to construct a graph. The constants
$a$ and $k$ are irrelevant to this issue. Changing $a$ just rescales the horizontal $r$ axis, and changing $k$ does the
same for the vertical $E$ axis. That means we can arbitrarily set $a=1$ and $k=1$, and construct the graph shown in the figure.
The points $r = \pm a$ are now simply $r=\pm 1$. From the graph, we can see that they're clearly minima. Physically, the
minimum at $r=-a$ can be interpreted as the same physical configuration of the molecule, but with the positions of the atoms
reversed. It makes sense that $r=-a$ behaves the same as $r=a$, since physically the behavior of the system has to be symmetric,
regardless of whether we view it from in front or from behind.

%%graph%% soln-lennard-jones func=x**12-2*x**6 format=eps xlo=-1.3 xhi=1.3 ylo=-2 yhi=8 x=r y=E ytic_spacing=2 more_space_below=6 more_space_above=10
\smallfig{soln-lennard-jones}{Problem \ref{hw:lennard-jones}.}

The other method of checking that $r=a$ is a minimum is to take the second derivative. As before, the values of $a$ and
$k$ are irrelevant, and can be set to 1. We then have
\begin{align*}
 \dot{E} &= -12r^{-13}+12r^{-7} \\
 \ddot{E} &= 156r^{-14}-84r^{-8}\eqquad.
\end{align*}
Plugging in $r=\pm 1$, we get a positive result, which confirms that the concavity is upward.

\hwsolnhdr{prove-n-extrema}

Since polynomials don't have kinks or endpoints in their graphs, the maxima and minima must be points where the
derivative is zero. Differentiation bumps down all the powers of a polynomial by one, so 
the derivative of a third-order polynomial is a second-order polynomial. A second-order polynomial
can have at most two real roots (values of $t$ for which it equals zero), which are given by the
quadratic formula. (If the number inside the square root in the quadratic formula is zero or negative,
there could be less than two real roots.) That means a third-order polynomial can have at most two
maxima or minima.

\hwsolnhdr{max-min-of-sum}

Since $f$, $g$, and $s$ are smooth and defined everywhere, any extrema they possess occur at places where their
derivatives are zero. The converse is not necessarily true, however; a place where the derivative is zero could
be a point of inflection. The derivative is additive, so if \emph{both} $f$ and $g$ have zero derivatives at a certain
point, $s$ does as well. Therefore in most cases, if $f$ and $g$ both have an extremum at a point, so will $s$.
However, it could happen that this is only a point of inflection for $s$, so in general, we can't conclude anything
about the extrema of $s$ simply from knowing where the extrema of $f$ and $g$ occur.

Going the other direction, we certainly can't infer anything about extrema of $f$ and $g$ from knowledge of $s$ alone.
For example, if $s(x)=x^2$, with a minimum at $x=0$, that tells us very little about $f$ and $g$. We could have, for example,
$f(x)=(x-1)^2/2-2$ and $g(x)=(x+1)^2/2+1$, neither of which has an extremum at $x=0$.

\hwsolnhdr{pyramidal-tent}

Considering $V$ as a function of $h$, with $b$ treated as a constant, we have for the slope
of its graph
\begin{align*}
  \dot{V} &= \frac{e_V}{e_h}\eqquad,\\
\intertext{so}
  e_V &= \dot{V}\cdot e_h \\
      &= \frac{1}{3}b e_h
\end{align*}

\hwsolnhdr{rocket-height}

Thinking of the rocket's height as a function of time, we can see that goal is to measure
the function at its maximum. The derivative is zero at the maximum, so the error incurred
due to timing is approximately zero. She should not worry about the timing error too much.
Other factors are likely to be more important, e.g., the rocket may not rise exactly
vertically above the launchpad.

\hwsolnhdr{sum-of-squares-coeff}
If $\dot{x}=n^2$, and $x$ is a polynomial in $n$, then we must have
$\dot{x}(n)=x(n)-x(n-1)=n^2$. If $x$ is a polynomial of order $k$, then
$x(n)$ and $x(n-1)$ both have $n^k$ terms with coefficients of 1, so 
$\dot{x}$ has no $n^k$ term. We want $\dot{x}$ to have a nonvanishing $n^2$ term,
so we must have $k\ge 3$. For $k>3$, it's easy to show that the $n^3$ term in
$x(n)-x(n-1)$ is nonzero, so we must have $k=3$. Let $x(n)=an^3+bn^2+\ldots$, where
$a$ is the coefficient that we want to prove is $1/3$, and $\ldots$ represents
lower-order terms. By the binomial theorem, we have $x(n-1)=an^3-3an^2+bn^2+\ldots$,
and subtracting this from $x(n)$ gives $\dot{x}(n)=3an^3+\ldots$. Since $3a=1$,
we have $a=1/3$.

\beginsolutions{2}

\hwsolnhdr{fourth-power}

\begin{align*}
  \frac{\der x}{\der t} &= \frac{(t+\der t)^4-t^4}{\der t} \\
                        &= \frac{4t^3\der t + 6t^2\der t^2 + 4t\der t^3+\der t^4}{\der t} \\
                        &= 4t^3 + \ldots\eqquad,
\end{align*}
where \ldots indicates infinitesimal terms.
The derivative is the standard part of this, which is $4t^3$.

\hwsolnhdr{derivative-of-cos}

\begin{equation*}
  \frac{\der x}{\der t} = \frac{\cos(t+\der t)-\cos t}{\der t}
\end{equation*}
The identity $\cos(\alpha+\beta)=\cos\alpha\cos\beta-\sin\alpha\sin\beta$ then gives
\begin{equation*}
  \frac{\der x}{\der t} = \frac{\cos t \: \cos \der t - \sin t \: \sin \der t - \cos t}{\der t}\eqquad.\\
\end{equation*}
The small-angle approximations  $\cos \der t\approx 1$ and $\sin \der t\approx \der t$
result in
\begin{align*}
\frac{\der x}{\der t}   &= \frac{-\sin t \der t}{\der t} \\
                        &= -\sin t\eqquad.
\end{align*}

\hwsolnhdr{infinite-subtraction}

\begin{tabular}{ll}
$H$ & $\sqrt{H+1}-\sqrt{H-1}$ \\
$1000$ & .032 \\
$1000,000$ & 0.0010 \\
$1000,000,000$ & 0.00032 
\end{tabular}

The result is getting smaller and smaller, so it seems reasonable to guess that if $H$ is infinite,
the expression gives an infinitesimal result.

\hwsolnhdr{infinitesimal-sqrt}

\begin{tabular}{ll}
$\der x$ & $\sqrt{\der x}$ \\
.1 & .32 \\
.001 & .032 \\
.00001 & .0032
\end{tabular}

The square root is getting smaller, but is not getting smaller as fast as the number itself.
In proportion to the original number, the square root is actually getting \emph{bigger}. It
looks like $\sqrt{\der x}$ is infinitesimal, but it's still infinitely big compared to
$\der x$. This makes sense, because $\sqrt{\der x}$ equals $\der x^{1/2}$.
we already knew that $\der x^0$, which equals 1, was
infinitely big compared to $\der x^1$, which equals $\der x$.
In the hierarchy of infinitesimals, $\der x^{1/2}$ fits in between $\der x^0$
and  $\der x^1$.

\hwsolnhdr{transfer}\\*
Statements (a)-(d), and (f)-(g) are all valid for the hyperreals, because they meet the test of being
directly translatable, without having to interpret the meaning of things like particular
subsets of the reals in the context of the hyperreals.

Statement (e), however, refers to the
rational numbers, a particular subset of the reals, and that means that it can't be
mindlessly translated into a statement about the hyperreals, unless we had figured out
a way to translate the set of rational numbers into some corresponding subset of the
hyperreal numbers like the hyperrationals! This is not the type of statement that the
transfer principle deals with. The statement is not true if we try to change ``real''
to ``hyperreal'' while leaving ``rational'' alone; for example, it's not true that there's
a rational number that lies between the hyperreal numbers 0 and $0+\der x$, where $\der x$
is infinitesimal.

\hwsolnhdr{parallel-resistance}
If $R_1$ is finite and $R_2$ infinite, then $1/R_2$ is infinitesimal,
$1/R_1+1/R_2$ differs infinitesimally from $1/R_1$, and the combined resistance $R$ differs infinitesimally
from $R_1$. Physically, the second pipe is blocked or too thin to carry any significant flow,
so it's as though it weren't present.

If $R_1$ is finite and $R_2$ is infinitesimal, then $1/R_2$ is infinite, $1/R_1+1/R_2$ is
also infinite, and the combined resistance $R$ is infinitesimal. It's so easy for water to
flow through $R_2$ that $R_1$ might as well not be present. In the context of electrical circuits
rather than water pipes, this is known as a short circuit.

\hwsolnhdr{velocity-addition-infinitesimals}
The velocity addition is only interesting if the infinitesimal velocities $u$ and $v$ are comparable
to one another, i.e., their ratio is finite. Let's write $\epsilon$ for the size of these infinitesimals,
so that both $u$ and $v$ can be written as $\epsilon$ multiplied by some finite number.
Then $1+uv$ differs from 1 by an amount that is on the order of $\epsilon^2$, which is infinitesimally
small compared to $\epsilon$. The same then holds true for $1/(1+uv)$ as well. The result of velocity
addition $(u+v)/(1+uv)$ is then $u+v+\ldots$, where $\ldots$ represents quantities of order
$\epsilon^3$, which are amount to a correction that is infinitesimally small compared to the nonrelativistic
result $u+v$.

\hwsolnhdr{hundredth}
This would be a horrible problem if we had to expand this as a polynomial with 101 terms,
as in chapter 1! But now we know the chain rule, so it's easy. The derivative is
\begin{equation*}
  \left[100(2x+3)^{99}\right][2]\eqquad,
\end{equation*}
where the first factor in brackets is the derivative of the function on the outside, and
the second one is the derivative of the ``inside stuff.'' Simplifying a little, the
answer is $200(2x+3)^{99}$.


\hwsolnhdr{one-and-two-hundred}\\*
Applying the product rule, we get
\begin{equation*}
  (x+1)^{99}(x+2)^{200}+  (x+1)^{100}(x+2)^{199}\eqquad.
\end{equation*}
(The chain rule was also required, but in a trivial way --- for both of
the factors, the derivative of the ``inside stuff'' was one.)

\hwsolnhdr{ex-chain}\\*
The derivative of $e^{7x}$ is $e^{7x}\cdot 7$, where the first factor is the
derivative of the outside stuff (the derivative of a base-$e$ exponential is
just the same thing), and the second factor is the derivative of the inside stuff.
This would normally be written as $7e^{7x}$.

The  derivative of the second function is $e^{e^x}e^x$, with the second exponential factor coming
from the chain rule.

\hwsolnhdr{sinusoidal}\\*
We need to put together three different ideas here: (1) When a function to be
differentiated is multiplied by a constant, the constant just comes along for the ride.
(2) The derivative of the sine is the cosine. (3) We need to use the chain rule.
The result is $-ab\cos(bx+c)$.

\hwsolnhdr{integrate-sinusoidal}\\*
If we just wanted to fine the integral of $\sin x$, the answer would be $-\cos x$ (or
$-\cos x$ plus an arbitrary constant), since the derivative would be $-(-\sin x)$, which
would take us back to the original function. The obvious thing to guess for the
integral of $a\sin(bx+c)$ would therefore be $-a\cos(bx+c)$, which almost works,
but not quite. The derivative of this function would be $ab\sin(bx+c)$, with the
pesky factor of $b$ coming from the chain rule. Therefore what we really wanted was
the function $-(a/b)\cos(bx+c)$.

\hwsolnhdr{square-three-times}\\*
The chain rule gives
\begin{equation*}
  \frac{\der}{\der x} ((x^2)^2)^2 = 2((x^2)^2)(2(x^2))(2x) = 8x^7\eqquad,
\end{equation*}
which is the same as the result we would have gotten by differentiating $x^8$.

\hwsolnhdr{max-range}\\*
To find a maximum, we take the derivative and set it equal to zero. The whole factor
of $2v^2/g$ in front is just one big constant, so it comes along for the ride. To differentiate
the factor of $\sin\theta\:\cos\theta$, we need to use the chain rule, plus the fact that
the derivative of sin is cos, and the derivative of cos is $-\sin$.
\begin{gather*}
  0 = \frac{2v^2}{g} (\cos\theta\:\cos\theta+\sin\theta(-\sin\theta)) \\
  0 = \cos^2\theta-\sin^2\theta \\
  \cos\theta = \pm \sin\theta \\
\intertext{We're interested in angles between, 0 and 90 degrees, for which both the sine and
the cosine are positive, so}
  \cos\theta = \sin\theta \\
  \tan\theta = 1\\
  \theta = 45\degunit\eqquad.
\end{gather*}
To check that this is really a maximum, not a minimum or an inflection point, we could resort
to the second derivative test, but we know the graph of $R(\theta)$ is zero at $\theta=0$
and $\theta=90\degunit$, and positive in between, so this must be a maximum.

\hwsolnhdr{cosh}\\*
Taking the derivative and setting it equal to zero, we have $\left(e^x-e^{-x}\right)/2=0$, so $e^x=e^{-x}$, which
occurs only at $x=0$. The second derivative is $\left(e^x+e^{-x}\right)/2$ (the same as the original function),
which is positive for all $x$, so the function is everywhere concave up, and this is a minimum.

\hwsolnhdr{sin-sin-sin}\\*
There are no kinks, endpoints, etc., so
extrema will occur only in places where the derivative is zero.
Applying the chain rule, we find the derivative to be $\cos(\sin(\sin x))\cos(\sin x)\cos x$.
This will be zero if any of the three factors is zero. We have $\cos u=0$ only when $|u| \ge \pi/2$,
and $\pi/2$ is greater than 1, so it's not possible for either of the first two factors to equal zero.
The derivative will therefore equal zero if and only if $\cos x=0$, which happens in the same places
where the derivative of $\sin x$ is zero, at $x=\pi/2+\pi n$, where $n$ is an integer.
%
%%graph%% soln-sin-sin-sin func=sin(sin(sin(x))) format=eps xlo=-10 xhi=10 ylo=-1 yhi=1 ytic_spacing=1 xtic_spacing=10
\smallfig{soln-sin-sin-sin}{Problem \ref{hw:sin-sin-sin}.}

This essentially completes the required demonstration, but there is one more technical issue, which
is that it's conceivable that some of these could
be points of inflection. Constructing a graph of $\sin(\sin(\sin x))$ gives us the necessary
insight to see that this can't be the case. The function essentially looks like the sine function, but
its extrema have been ``shaved down'' a little, giving them slightly flatter tips that don't quite extend
out to $\pm 1$. It's therefore fairly clear that these aren't points of inflection.
To prove this more rigorously, we could take the second derivative and show that it was
nonzero at the places where the first derivative is zero. That would be messy. A less tedious argument is
as follows. We can tell from its formula that the function is \emph{periodic},\index{periodic function} i.e., it has the property that
$f(x+\ell)=f(x)$, for $\ell=2\pi$. This follows because the innermost sine function is periodic, and the outer layers only
depend on the result of the inner layer. Therefore all the points of the form $\pi/2+2\pi n$
have the same behavior. Either they're all maxima or they're all points of inflection. But clearly a
function can't oscillate back and forth without having any maxima at all, so they must all be maxima. A
similar argument applies to the minima.


\hwsolnhdr{absval-derivative}\\*
The function $f$ has a kink at $x=0$, so it has no uniquely defined tangent line there, and its derivative
at that point is undefined. In terms of infinitesimals, positive values of $\der x$ give
$\der f/\der x=(\der x+\der x)/\der x=2$, while negative ones give $\der f/\der x=(-\der x+\der x)/\der x=0$.
Since the standard part of the quotient $\der y/\der x$ depends on the specific value of $\der x$, the derivative is
undefined.

The function $g$ has no kink at $x=0$. The graph of $x|x|$ looks like two half-parabolas glued together, and since
both of them have slopes of 0 at $x=0$, the slope of the tangent line is well defined, and is zero. In terms of
infinitesimals, $\der g/\der y$ is the standard part of $|\der x|+1$, which is 1.

\hwsolnhdr{air-res-v}\\*
(a) As suggested, let $c=\sqrt{g/A}$, so that  $d = A \ln\cosh ct=A\ln\left(e^{ct}+e^{-ct}\right)$.
Applying the chain rule, the velocity is
\begin{equation*}
  A\frac{ce^{ct}-ce^{-ct}}{\cosh ct}\eqquad.
\end{equation*}
(b) The expression can be rewritten as $Ac\tanh ct$.\\
(c) For large $t$, the $e^{-ct}$ terms become negligible, so the velocity is $Ace^{ct}/e^{ct}=Ac$.
(d) From the original expression, $A$ must have units of distance, since the logarithm is unitless.
Also, since $ct$ occurs inside a function, $ct$ must be unitless, which means that $c$ has units
of inverse time. The answers to parts b and c get their units from the factors of $Ac$, which have
units of distance multiplied by inverse time, or velocity.

\hwsolnhdr{derivative-of-tan}\\*
Since I've advocated not memorizing the quotient rule, I'll do this one from first principles,
using the product rule.
\begin{align*}
  \frac{\der}{\der \theta}& \tan\theta \\
        &=   \frac{\der}{\der \theta}\left(\frac{\sin\theta}{\cos\theta}\right) \\
        &=   \frac{\der}{\der \theta}\left[\sin\theta\left(\cos\theta\right)^{-1}\right] \\
        &=   \cos\theta\left(\cos\theta\right)^{-1}+(\sin\theta)(-1)(\cos\theta)^{-2}(-\sin\theta)\\
        &=   1+\tan^2\theta
\end{align*}
(Using a trig identity, this can also be rewritten as $\sec^2\theta$.)

\hwsolnhdr{cube-root}\\*
Reexpressing $\sqrt[3]{x}$ as $x^{1/3}$, the derivative is $(1/3)x^{-2/3}$.

\hwsolnhdr{thompson-sqrts}\\*
(a) Using the chain rule, the derivative of $(x^2+1)^{1/2}$ is $(1/2)(x^2+1)^{-1/2}(2x)=x(x^2+1)^{-1/2}$.\\
(b) This is the same as a, except that the 1 is replaced with an $a^2$, so the answer is $x(x^2+a^2)^{-1/2}$. The idea
would be that $a$ has the same units as $x$.\\
(c) This can be rewritten as $(a+x)^{-1/2}$, giving a derivative of $(-1/2)(a+x)^{-3/2}$.\\
(d) This is similar to c, but we pick up a factor of $-2x$ from the chain rule, making the result
$ax(a-x^2)^{-3/2}$.

\hwsolnhdr{logarithmy}\\*
By the chain rule, the result is $2/(2t+1)$.

\hwsolnhdr{unnecessary-prod}\\*
Using the product rule, we have
\begin{equation*}
  \left(\frac{\der}{\der x}3\right)\sin x + 3\left(\frac{\der}{\der x}\sin x\right)\eqquad,
\end{equation*}
but the derivative of a constant is zero, so the first term goes away, and we get $3\cos x$, which
is what we would have had just from the usual method of treating multiplicative constants.


\hwsolnhdr{gamma}\\*
\restartLineNumbers
\begin{Code}
  \ii N(Gamma(2))
  \oo{1}
  \ii N(Gamma(2.00001))
  \oo{1.0000042278}
  \ii N( (1.0000042278-1)/(.00001) )
  \oo{0.4227799998}
\end{Code}
Probably only the first few digits of this are reliable.

\hwsolnhdr{cylinder}\\*
The area and volume are
\begin{align*}
  A &= 2\pi r \ell + 2 \pi r^2 \\
\intertext{and}
  V &= \pi r^2 \ell\eqquad.
\end{align*}
The strategy is to use the equation for $A$, which is a constant,
to eliminate the variable $\ell$, and then maximize $V$ in terms of $r$.
\begin{gather*}
  \ell = (A-2\pi r^2)/2\pi r \\
\intertext{Substituting this expression for $\ell$ back into the equation for $V$,}
  V = \frac{1}{2}rA-\pi r^3\eqquad.
\intertext{To maximize this with respect to $r$, we take the derivative and set it
equal to zero.}
  0 = \frac{1}{2}A-3\pi r^2 \\
  A = 6\pi r^2 \\
  \ell = (6\pi r^2-2\pi r^2)/2\pi r \\
  \ell = 2r
\end{gather*}
In other words, the length should be the same as the diameter.

\hwsolnhdr{relativistic-ke}\\*
(a) We can break the expression down into three factors: the constant $m/2$ in front,
the nonrelativistic velocity dependence $v^2$, and the relativistic correction
factor $(1-v^2/c^2)^{-1/2}$. Rather than substituting in $at$ for $v$, it's a little less
messy to calculate $\der K/\der t=(\der K/\der v)(\der v/\der t)=a\der K/\der v$. Using the product rule, we have
\begin{align*}
  \frac{\der K}{\der t} &= a\cdot\frac{1}{2}m\left[2v\left(1-{\textstyle \frac{v^2}{c^2}}\right)^{-1/2}\right.\\
                              &\left.+v^2\cdot{\textstyle \left(-\frac{1}{2}\right)}\left(1-{\textstyle \frac{v^2}{c^2}}\right)^{-3/2}
                                        {\textstyle \left(-\frac{2v}{c^2}\right)}\right] \\
             &=  ma^2t\left[\left(1-{\textstyle \frac{v^2}{c^2}}\right)^{-1/2}\right.\\
                              &\left.+\frac{v^2}{2c^2}\left(1-{\textstyle \frac{v^2}{c^2}}\right)^{-3/2}\right]
\end{align*}
(b) The expression $ma^2t$ is the nonrelativistic (classical) result, and has the correct units of
kinetic energy divided by time. The factor in square brackets is the relativistic correction,
which is unitless.\\
(c) As $v$ gets closer and closer to $c$, the expression $1-v^2/c^2$ approaches zero, so both the
terms in the relativistic correction blow up to positive infinity.

\hwsolnhdr{log-neg}\\*
We already know it works for positive $x$, so we only need to check it for negative $x$.
For negative values of $x$, the chain rule tells us that the derivative is $1/|x|$,
multiplied by $-1$, since $\der|x|/\der x=-1$. This gives $-1/|x|$, which is the same
as $1/x$, since $x$ is assumed negative.

\hwsolnhdr{odd-even}\\*
Since $f(x)=f(-x)$, 
\begin{equation*}
  \frac{\der f(x)}{\der x} =   \frac{\der f(-x)}{\der x}\eqquad.
\end{equation*}
But by the chain rule, the right-hand side equals $-f'(x)$, as claimed.

\hwsolnhdr{neg-power-trick}\\*
Let $f=\der x^k/\der x$ be the unknown function. Then
\begin{align*}
  1 &= \frac{\der x}{\der x} \\
    &= \frac{\der }{\der x} \left(x^kx^{-k+1}\right) \\
    &= fx^{-k+1}+x^k(-k+1)x^{-k}\eqquad,
\end{align*}
where we can use the ordinary rule for derivatives of powers on $x^{-k+1}$, since $-k+1$ is positive. Solving for
$f$, we have the desired result.

\hwsolnhdr{nonstandard-plane}
Since the parallel postulate can be expressed in terms of algebra through Cartesian geometry, the transfer principle
tells us that it holds for F as well. But G is defined in terms of the finite hyperreals, so statements about E
don't carry over to statements about G simply by replacing ``real'' with ``hyperreal,'' and the transfer principle
does not guarantee that the parallel postulate applies to G. 

In fact, it is easy to find a counterexample in G.
Let $\epsilon$ be an infinitesimal number. Consider the lines with equations $y=1$ and $y=1+\epsilon x$. Neither
of these intersects the $x$ axis.

No, it is not valid to associate only E with the plane described by Euclid's axioms. All of Euclid's axioms hold
equally well in F. F is referred to as a nonstandard model of Euclid's axioms. It has the same relation to standard
Euclidean geometry as the hyperreals have to the reals. If we want to make up a set of axioms that describes E and
can't describe F, then we need to add an additional axiom to Euclid's set. An example of such an axiom would be
an axiom stating that given any two line segments with lengths $\ell_1$ and $\ell_2$, there exists some integer
$n$ such that $n\ell_1>\ell_2$. Note that although this axiom holds in E, the transfer principle cannot be used
to show that it holds in F --- it is false in F. The transfer principle doesn't apply because the transfer
principle doesn't apply to statements that include phrases such as ``for any integer.''

\hwsolnhdr{nines-forever}

The normal definition of a repeating decimal such as $0.999\ldots$ is that it is the \emph{limit} of the sequence
$0.9$, $0.99$, $\ldots$, and the limit is a real number, by definition. $0.999\ldots$ equals 1.
However, there is an intuition that the limiting process $0.9$, $0.99$, $\ldots$ ``never quite gets
there.'' This intuition can, in fact, be formalized in the construction described beginning on
page \pageref{detour:transfer-true}; we can define a hyperreal number based on the sequence
$0.9$, $0.99$, $\ldots$, and it is a number infinitesimally less than one. This is not, however,
the normal way of defining the symbol $0.999\ldots$, and we probably wouldn't want to change the
definition so that it was. If it was, then $0.333\ldots$ would not equal $1/3$.

\hwsolnhdr{chain-rule-units}

Converting these into Leibniz notation, we find
\begin{align*}
  \frac{\der f}{\der x} &=   \frac{\der g}{\der h}
\intertext{and}
  \frac{\der f}{\der x} &=   \frac{\der g}{\der h}\cdot h\eqquad.
\end{align*}
To prove something is not true in general, it suffices to find one counterexample. Suppose
that $g$ and $h$ are both unitless, and $x$ has units of seconds. The value of $f$ is defined by
the output of $g$, so $f$ must also be unitless. Since $f$ is unitless, $\der f/\der x$ has units
of inverse seconds (``per second''). But this doesn't match the units of either of the proposed expressions,
because they're both unitless.
The correct chain rule, however, works. In the equation
\begin{equation*}
  \frac{\der f}{\der x} =   \frac{\der g}{\der h}\cdot \frac{\der h}{\der x}\eqquad,
\end{equation*}
the right-hand side consists of a unitless factor multiplied by a factor with units of
inverse seconds, so its units are inverse seconds, matching the left-hand side.

\hwsolnhdr{resonance}

We can make life a lot easier by observing that the function $s(f)$ will be maximized when the
expression inside the square root is minimized. Also, since $f$ is squared every time it occurs,
we can change to a variable $x=f^2$,
and then once the optimal value of $x$ is found we can take its square root in order to find
the optimal $f$. The function to be optimized is then
\begin{equation*}
  a(x-f_\zu{o}^2)^2+bx\eqquad.
\end{equation*}
Differentiating this and setting the derivative equal to zero, we find
\begin{equation*}
  2a(x-f_\zu{o}^2)+b = 0\eqquad,
\end{equation*}
which results in $x=f_\zu{o}^2-b/2a$, or
\begin{equation*}
  f = \sqrt{f_\zu{o}^2-b/2a}\eqquad,
\end{equation*}
(choosing the positive root, since $f$ represents a frequencies, and frequencies are positive by definition).
Note that the quantity inside the square root involves the square of a frequency, but then we take its
square root, so the units of the result turn out to be frequency, which makes sense. We can see that
if $b$ is small, the second term is small, and the maximum occurs very nearly at
$f_\zu{o}$. 

There is one subtle issue that was glossed over above, which is that the graph on page \pageref{fig:resonance}
shows \emph{two} extrema: a minimum at $f=0$ and a maximum at $f>0$. What happened to the $f=0$ minimum?
The issue is that I was a little sloppy with the change of variables. Let $I$ stand for the quantity inside
the square root in the original expression for $s$. Then by the chain rule,
\begin{equation*}
  \frac{\der s}{\der f} =   \frac{\der s}{\der I} \cdot \frac{\der I}{\der x} \cdot \frac{\der x}{\der f} .
\end{equation*}
We looked for the place where $\der I/\der x$ was zero, but $\der s/\der f$ could also be zero if one of the
other factors was zero. This is what happens at $f=0$, where $\der x/\der f=0$.

\hwsolnhdr{near-focal-point}

\begin{align*}
  y &= \left(\frac{1}{f}-\frac{1}{x}\right)^{-1} \\
    &= \left(\frac{1}{f}-\frac{1}{f+\der x}\right)^{-1} \\
    &= f\left(1-\frac{1}{1+\der x/f}\right)^{-1} \\
\intertext{Applying the geometric series $1/(1+r)=1+r+r^2+\ldots$,}
  y &\approx f\left(1-\left(1-\frac{\der x}{f}\right)\right)^{-1} \\
    &= \frac{f^2}{\der x} 
\end{align*}

As checks on our result, we note that the units work out correctly (meters squared divided by
meters give meters), and that the result is indeed large, since we divide by the small quantity $\der x$.

\hwsolnhdr{one-to-power-infinity}
One way to evaluate an expression like $a^b$ is by using the identity
$a^b=e^{b\ln a}$. If we try to substitute $a=1$ and $b=\infty$,
we get $e^{\infty\cdot 0}$, which has an indeterminate form inside
the exponential. One way to express the idea is that
if there is even the tiniest error in the value of $a$, the value of $a^\infty$
can have any positive value.

\beginsolutions{3}

\hwsolnhdr{limit-of-sum}

(a) The Weierstrass definition requires that if we're given a particular $\epsilon$, and we be able to find a $\delta$ so small that $f(x)+g(x)$
differs from $F+G$ by at most $\epsilon$ for $|x-a|<\delta$. But the Weierstrass definition also tells us that given $\epsilon/2$, we can find
a $\delta$ such that $f$ differs from $F$ by at most $\epsilon/2$, and likewise for $g$ and $G$. The amount by which $f+g$ differs from $F+G$
is then at most $\epsilon/2+\epsilon/2$, which completes the proof.

(b) Let $\der x$ be infinitesimal. Then the definition of the limit in terms of infinitesimals says that the standard part of $f(a+\der x)$
differs at most infinitesimally from $F$, and likewise for $g$ and $G$. This means that $f+g$ differs from $F+G$ by the sum of two infinitesimals,
which is itself an infinitesimal, and therefore the standard part of $f+g$ evaluated at $x+\der x$ equals $F+G$, satisfying the definition.

\hwsolnhdr{limits-with-exp-one-over-x}

The shape of the graph can be found by considering four cases:
 large negative $x$, small negative $x$, small positive $x$, and large positive $x$.
In these four cases, the function is respectively close to 1, large, small, and close to 1.

%%graph%% soln-limits-with-exp-one-over-x func=exp(-1/x) format=eps xlo=-3 xhi=3 ylo=0 yhi=8 x=t y=x ytic_spacing=4 more_space_below=6 more_space_above=10
\smallfig{soln-limits-with-exp-one-over-x}{Problem \ref{hw:limits-with-exp-one-over-x}.}

The four limits correspond to the four cases described above.

\hwsolnhdr{granville-lhospital}
All five of these can be done using l'H\^{o}pital's rule:

\begin{align*}
  \lim_{s\rightarrow 1} & \frac{s^3-1}{s-1} = \lim \frac{3s^2}{1} = 3 \\
  \lim_{\theta\rightarrow 0} & \frac{1-\cos\theta}{\theta^2} = \lim \frac{\sin\theta}{2\theta} = \lim \frac{\cos\theta}{2} = \frac{1}{2} \\
  \lim_{x\rightarrow \infty} & \frac{5x^2-2x}{x} = \lim \frac{10x-2}{1} = \infty \\
  \lim_{n\rightarrow \infty} & \frac{n(n+1)}{(n+2)(n+3)} = \lim \frac{n^2+\ldots}{n^2+\ldots}  = \lim \frac{2n+\ldots}{2n+\ldots} = \lim \frac{2}{2} = 1 \\
  \lim_{x\rightarrow \infty} & \frac{ax^2+bx+c}{dx^2+ex+f} = \lim \frac{2ax+\ldots}{2dx+\ldots} = \lim \frac{2a}{2d} = \frac{a}{d}
\end{align*}
In examples 2, 4, and 5, we differentiate more than once in order to get an expression that can be evaluated by
substitution. In 4 and 5, \ldots represents terms that we anticipate will go away after the second differentiation.
Most people probably would not bother with l'H\^{o}pital's rule for 3, 4, or 5, being content merely to observe
the behavior of the highest-order term, which makes the limiting behavior obvious. Examples 3, 4, and 5 can also
be done rigorously without l'H\^{o}pit rule, by algebraic manipulation;
we divide on the top and bottom by the highest power of the variable, giving an expression that is no longer
an indeterminate form $\infty/\infty$.

\hwsolnhdr{lhospital-cos-exp}

Both numerator and denominator go to zero, so we can apply l'H\^{o}pital's rule. Differentiating top and bottom gives
$(\cos x-x\sin x)/(-\ln 2 \cdot 2^x)$, which equals $-1/ln 2$ at $x=0$. To check this numerically, we plug 
$x=10^{-3}$ into the original expression. The result is $-1.44219$, which is very close to $-1/ln 2=-1.44269\ldots$.

\hwsolnhdr{lhospital-applied-incorrectly}

L'H\^{o}pital's rule only works when both the numerator and the denominator go to zero.

\hwsolnhdr{lhospital-twice}
Applying l'H\^{o}pital's rule once gives
\begin{equation*}
  \lim_{u\rightarrow 0} \frac{2u}{e^u-e^{-u}}\eqquad,
\end{equation*}
which is still an indeterminate form. Applying the rule a second time, we get
\begin{equation*}
  \lim_{u\rightarrow 0} \frac{2}{e^u+e^{-u}} = 1\eqquad.
\end{equation*}
As a numerical check, plugging $u=0.01$ into the original expression results
in $0.9999917$.

\hwsolnhdr{lhospital-not-at-zero}
L'H\^{o}pital's rule gives $\cos t/1\rightarrow -1$. Plugging in $t=3.1$
gives -0.9997.

\hwsolnhdr{lhospital-at-infinity}
Let $u=1/x$. Then
\begin{equation*}
  \frac{\der f/\der x}{\der g/\der x}  =   \frac{\der f/\der u}{\der g/\der u}\eqquad,
\end{equation*}
simply by algebraic manipulation of the infinitesimals. (If we want to interpret these quantities as
derivatives, then our notational convention is that they stand for the standard parts of the quotients
of the infinitesimals, in which case the equality is only for the standard parts.) This equality  holds not just in the limit
but everywhere that the functions are differentiable. The expression on the left is the thing whose limit we're trying to
prove equals $\lim f/g$. The right-hand side is equal to $\lim f/g$ by the previously established form of l'H\^{o}pital's rule.

\hwsolnhdr{line-is-continuous}
By the  definition of
continuity in terms of infinitesimals, the function is continuous, because an infinitesimal change $\der x$ leads
to a change $\der y=a\der x$ in the output of the function which is likewise infinitesimals. (This depends on the
fact that $a$ is assumed to be real, which implies that it is finite.)

Continuity in terms of the Weierstrass limit holds because we can take $\delta=\epsilon/a$.


\beginsolutions{4}

\hwsolnhdr{integrate-num}\\*
\restartLineNumbers
\begin{Code}
  a := 0;
  b := 1;
  H := 1000;
  dt := (b-a)/H;
  sum := 0;
  t := a;
  While (t<=b) [
    sum := N(sum+Exp(x^2)*dt);
    t := N(t+dt);
  ];
  Echo(sum);
\end{Code}
The result is 1.46.

%%graph%% soln-integrate-sin-cancel func=sin(x) format=eps xlo=0 xhi=6.282 ylo=-1 yhi=1 ytic_spacing=1 more_space_below=6 more_space_above=10
\smallfig{soln-integrate-sin-cancel}{Problem \ref{hw:integrate-sin-cancel}.}

\hwsolnhdr{integrate-sin-cancel}\\*
The derivative of the cosine is minus the sine, so to get a function whose
derivative is the sine, we need minus the cosine.
\begin{align*}
  \int_0^{2\pi} & \sin x \der x \\
    &= \left.(-\cos x)\right|_0^{2\pi} \\
    &= (-\cos 2\pi)-(-\cos 0) \\
    &= (-1)-(-1) \\
    &= 0
\end{align*}

As shown in  figure \figref{soln-integrate-sin-cancel}, the graph has equal amounts of area above and below the $x$ axis.
The area below the axis counts as negative area, so the total is zero.

\hwsolnhdr{estimate-then-integrate}\\*
%%graph%% soln-estimate-then-integrate func=-x**2+2*x format=eps xlo=0 xhi=2 ylo=0 yhi=1 ytic_spacing=1 more_space_below=6 more_space_above=10
\smallfig{soln-estimate-then-integrate}{Problem \ref{hw:estimate-then-integrate}.}

The rectangular area of the graph is 2, and the area under the curve fills a little more than half of that, so let's guess 1.4.

\begin{align*}
  \int_0^2 -x^2+2x &= \left.\left(-\frac{1}{3}x^3+x^2\right)\right|_0^2 \\
             &= (-8/3+4)-(0) \\
             &= 4/3
\end{align*}

This is roughly what we were expecting from our visual estimate.

\hwsolnhdr{average-sine}\\*
Over this interval, the value of the $\sin$ function varies from 0 to 1, and
it spends more time above 1/2 than below it, so we expect the average to be
somewhat greater than 1/2.
The exact result is
\begin{align*}
  \overline{\sin} &= \frac{1}{\pi-0}\int_0^\pi \sin x\der x \\
             &= \frac{1}{\pi}\left.(- \cos x)\right|_0^\pi \\
             &= \frac{1}{\pi}[-\cos\pi-(-\cos 0)] \\
             &= \frac{2}{\pi}\eqquad,
\end{align*}
which is, as expected, somewhat more than 1/2.

\hwsolnhdr{continuity-mean-value}\\*
Consider a function $y(x)$ defined on the interval from $x=0$ to 2 like this:
\begin{equation*}
  y(x) = 
    \begin{cases}
      -1  & \text{if $0\le x \le 1$}\\
      1  & \text{if $1< x \le 2$}
    \end{cases}
\end{equation*}
The mean value of $y$ is zero, but $y$ never equals zero.

\hwsolnhdr{fund-thm-assumption}\\*
Let $\dot{x}$ be defined as
\begin{equation*}
  \dot{x}(t) = 
    \begin{cases}
      0  & \text{if $t < 0$}\\
      1  & \text{if $t \ge 0$}
    \end{cases}
\end{equation*}
Integrating this function up to $t$ gives
\begin{equation*}
  x(t) = 
    \begin{cases}
      0  & \text{if $t \le 0$}\\
      t  & \text{if $t \ge 0$}
    \end{cases}
\end{equation*}
The derivative of $x$ at $t=0$ is undefined, and therefore
integration followed by differentiation doesn't recover
the original function $\dot{x}$.

\hwsolnhdr{int-nested-square-roots}
First we put the integrand into the more familiar and convenient form $cx^p$, whose integral is $(c/(p+1))x^{p+1}$.
$\sqrt{bx\sqrt{x}}=b^{1/2}x^{3/4}$. Applying the general rule, the result is $(4/7)b^{1/2}x^{7/4}$.

\hwsolnhdr{bogus-odd-even-proof}
The claim is false for indefinite integrals, since indefinite integrals can have a constant of integration.
So, for example, a possible indefinite integral of $x^2$ is $x^3/3+7$, which is neither even nor odd.
The fundamental theorem doesn't even refer to indefinite integrals, which are simply \emph{defined} 
through inverse differentiation.

Let's fix the claim by changing $g$ to a definite integral, $g(x)=\int_0^x f(u)\der u$.
The claim is now true. However, the proof still doesn't quite work. We've established that
all odd functions have even derivatives, but we haven't ruled out possibilities such as
functions that are neither even nor odd, but that have even derivatives.

\beginsolutions{5}
\hwsolnhdr{generalize-integ-by-parts}

It's pretty trivial to generalize from $e$ to $b$. If we write $b^x$ as $e^{x\ln b}$, then
we can substitute $u=x \ln b$ and reduce the $b\ne e$ case to $b=e$.

The generalization of the exponent of $x$ from 2 to $a$ is less straightforward. To do it
with $a=2$, we needed two integrations by parts, so clearly if we wanted to do a case with
$a=37$, we could do it with 37 integrations by parts. However, we would  have no easy way to
write down the complete answer without going through the whole tedious calculation. Furthermore,
this is only going to work if $a$ is a positive integer.

\hwsolnhdr{exp-of-power-doable}
The obvious substitution is $u=x^p$, which leads to the form
$\int e^u u^{1/p-1}\der u$. If the exponent $1/p-1$ equals a nonnegative integer $n$, then
through $n$ integrations by parts, we can reduce this to the form $\int e^x\der x$.
This requires $p=1$, 1/2, 1/3, \ldots

\hwsolnhdr{vladimir-arnold}
This is a mess if attacked by brute force. The trick is to reexpress the function using partial fractions:
\begin{equation*}
  \frac{x^2+1}{x^3-x} = \frac{x^2+1}{2(x+1)}+\frac{x^2+1}{2(x-1)}-\frac{x^2+1}{x}\eqquad.
\end{equation*}
Writing $u=x+1$ and $v=x-1$, this becomes
\begin{equation*}
  u^{-1}+v^{-1}-x^{-1}+\ldots\eqquad,
\end{equation*}
where \ldots represents terms that will not survive multiple differentiations. Since $\der u/\der x=\der v/\der x=1$,
the chain rule tells us that differentiation with respect to $u$ or $v$ is the same as differentiation with respect
to $x$. The result is $100!(u^{-101}+v^{-101}-x^{-101})$, where the notation $100!$ means $1\times2\times\ldots100$.

\beginsolutions{6}

\hwsolnhdr{x-squared-two-to-x-improper}

The method of finding the indefinite integral is discussed in example \ref{eg:integrate-x-squared-times-exp} on p.~\pageref{eg:integrate-x-squared-times-exp}
and problem \ref{hw:generalize-integ-by-parts} on p.~\pageref{hw:generalize-integ-by-parts}. The result is
$-(\ln 2)^{-3}e^{-u}\left(-u^2-2u+2\right)$, where $u=-x\ln 2$. Plugging in the limits of integration, we obtain $2(\ln 2)^{-3}$.

\beginsolutions{7}

\hwsolnhdr{sequence-weierstrass}

We can define the sequence $f(n)$ as converging to $\ell$
if the following is true: for any real number $\epsilon$, there exists an integer
$N$ such that for all $n$ greater than $N$,
the value of $f$ lies within the range from $\ell-\epsilon$ to $\ell+\epsilon$.

\hwsolnhdr{bogus-series}

(a) The convergence of the series is defined in terms of the convergence of its partial sums,
which are 1, 0, 1, 0, \ldots In the notation used in the definition given in the solution to
problem \ref{hw:sequence-weierstrass} above, suppose we pick $\epsilon=1/4$. Then there is
clearly no way to choose any numbers $\ell$ and $N$ that would satisfy the definition,
for regardless of $N$, $\ell$ would have to be both greater than $3/4$ and less than $1/4$
in order to agree with the zeroes and ones that occur beyond the $N$th member of the sequence.

(b) As remarked on page \pageref{infinite-sum-warning}, the axioms of the real number system, such as
associativity, only deal with finite sums, not infinite ones. To see that absurd conclusions result
from attempting to apply them to infinite sums, consider that by the same type of argument we could group
the sum as $1+(-1+1)+(-1+1)+\ldots$, which would equal 1.

\hwsolnhdr{geometric-series-conv-integral}

The quantity $x^n$ can be reexpressed as $e^{n\ln x}$, where $\ln x$ is negative by hypothesis.
The integral of this exponential \emph{with respect to $n$} is a similar exponential with a constant
factor in front, and this converges as $n$ approaches infinity.

\hwsolnhdr{determine-convergence}

(a) Applying the integral test, we find that the integral of $1/x^2$ is $-1/x$, which converges as $x$ approaches infinity, so the series
converges as well.

(b) This is an alternating series whose terms approach zero, so it converges. However, the terms get small extremely
slowly, so an extraordinarily large number of terms would be required in order to get any kind of decent approximation
to the sum. In fact, it is impossible to carry out a straightforward numerical evaluation of this sum because it would
require such an enormous number of terms that the rounding errors would overwhelm the result.

(c) This converges by the ratio test, because the ratio of successive terms approaches 0.

(d) Split the sum into two sums, one for the 1103 term and one for the $26390k$. The ratio of the two factorials is always less than $4^{4k}$, so
discarding constant factors, the first sum is less than a geometric series with $x=(4/396)^4<1$, and must therefore converge. The second sum is less than a series of the form $kx^k$.
This one also converges, by the integral test. (It has to be integrated with respect to $k$, not $x$, and the integration can be done by parts.) Since both separate sums converge,
the entire sum converges. This bizarre-looking expression was formulated and shown to equal $1/\pi$ by the self-taught genius Srinivasa Ramanujan (1887-1920).

\hwsolnhdr{inconclusive-ratio-test}
E.g., $\sum_{n=0}^\infty \sin n$ diverges, but the ratio test won't establish that, because
the limit $\lim_{n\rightarrow\infty}|\sin(n+1)/\sin(n)|$ does not exist.

\hwsolnhdr{telescoping-series}
The $n$th term $a_n$ can be rewritten as $2/[n(n+1)]$, and using partial fractions this can be changed into
$2/n-2/(n+1)$. Let the partial sums be $s_n=\sum_1^n a_n$. For insight, let's write out $s_3$:
\begin{equation*}
  s_3 = \left(\frac{2}{1}-\frac{2}{2}\right)+\left(\frac{2}{2}-\frac{2}{3}\right)+\left(\frac{2}{3}-\frac{2}{4}\right)
\end{equation*}
This is called a telescoping series.\index{telescoping series}\index{series!telescoping}
The second part of one term cancels out with the first part of the next. Therefore we have 
\begin{equation*}
  s_3 = \frac{2}{1}-\frac{2}{4}\eqquad,
\end{equation*}
and in general
\begin{equation*}
  s_n = \frac{2}{1}-\frac{2}{n+1}\eqquad.
\end{equation*}
Letting $n\rightarrow\infty$, we find that the series sums to 2.

\hwsolnhdr{improper-integral-of-sin-x-squared}
Yes, it converges. To see this, consider that its graph consists of a series of peaks and valleys, each of
which is narrower than the last and therefore has less area. In fact, the width of these humps approaches
zero, so that the area approaches zero. This means that the integral can be represented
as a decreasing, alternating series that approaches zero, which must converge.

\hwsolnhdr{sum-of-iterated-sine}
There are certainly some special values of $x$ for which it does converge, such as 0 and $\pi$.
For a general value of $x$, however, things become more complicated. Let the $n$th term be given by the function $t(n)$.
$|t|$ converges to a limit, since the first application of the sine function brings us into the range $0\le |t|\le 1$,
and from then on, $|t|$ is decreasing and bounded below by 0. It can't approach a nonzero limit, for given such a limit $t^*$,
there would always be values of $t$ slightly greater than $t^*$ such that $\sin t$ was less than $t^*$. Therefore the terms
in the sum approach zero. This is necessary but not sufficient for the series to converge.

Once $t$ gets small enough, we can approximate the sine
using a Taylor series. Approximating the discrete function $t$ by a continuous one,
we have $\der t/\der n\approx -(1/6)t^3$, which can be rewritten as $t^{-3}\der t\approx -(1/6)\der n$. This is known as
separation of variables. Integrating, we find that at large values of $n$, where the constant of integration becomes negligible,
$t\approx \pm\sqrt{3/n}$. The sum diverges by the integral test. Therefore the sum diverges for all values of $x$ except for
multiples of $\pi$, which cause $t$ to hit zero immediately without passing through the region where the Taylor series
is a good approximation.

\hwsolnhdr{sum-of-n-squared-two-to-n}
Our first impression is that it must converge, since the $2^{-n}$ factor shrinks much more rapidly
than the $n^2$ factor. To prove this rigorously, we can apply the integral test. 
The relevant improper integral was carried out in problem \ref{hw:x-squared-two-to-x-improper}
on p.~\pageref{hw:x-squared-two-to-x-improper}.

Finding the sum is far more difficult, and there is no obvious technique that is guaranteed to work.
However, the integral test suggests an approach that does lead to a solution. The fact that the
\emph{indefinite} integral can be evaluated suggests that perhaps the partial sum
\begin{equation*}
  S_n = \sum_{j=0}^{n} j^2 2^{-j}
\end{equation*}
can also be evaluated. Furthermore, the fact that the integral was of the form $2^{-x}P(x)$, for some
polynomial $x$, suggests that perhaps $S_n$ is of the same form. Based on this conjecture, we
try to determine the unknown coefficients in $P(n)=an^2+bn+c$.
\begin{gather*}
  S_n-S_{n-1} = n^2 2^{-n} \\
  n^2 2^{-n} = 2^{-n}\left[-an^2+(4a-b)n-2a+2b-c\right]
\end{gather*}
Solving for $a$, $b$, and $c$ results in $P(n)=-n^2-4n-6$. This gives the correct value for the
difference $S_n-S_{n-1}$, but doesn't give $S_n=0$ as it should. But this is easy to fix simply
by changing the form of our conjectured partial sum slightly to $S_n=2^{-n}P(n)+k$, where
$k=6$. Evaluating $\lim_{n\rightarrow\infty}S_n$, we get 6.

\hwsolnhdr{open-question-about-sum}
The function $\cos^2$ averages to 1/2, so we might naively expect that $\cos^n$ would
average to about $2^{-n/2}$, in which case the sum would converge for any value of $p$ whatsoever.
But the average is misleading, because there are some ``lucky'' values of $n$ for which $\cos^2 n\approx 1$,
and these will have a disproportionate effect on the sum.
We know by the integral test that $\sum 1/n$ diverges,
but $\sum 1/n^2$ converges, so clearly if $p\ge 2$, then even these occasional ``lucky'' terms will
not cause divergence. 

What about $p=1$?
Suppose we have some value of $n$ for which
$\cos^2 n=1-\epsilon$, where $\epsilon$ is some small number. If this is to happen, then we must have
$n=k\pi+\delta$, where $k$ is an integer and $\delta$ is small, so that $\cos^2 n\approx 1-\delta^2$,
i.e., $\epsilon\approx\delta^2$. This occurs with a probability proportional to $\delta$, and the
resulting contribution to the sum is about $(1-\delta^2)^n/n$, which by the binomial theorem
is roughly of order of $1/n$ if $n\delta^2\sim1$. This happens with probability $\sim n^{-1/2}$,
so the expected value of the $n$th term is $\sim n^{-3/2}$. Since $\sum n^{-3/2}$ converges
by the integral test,
this suggests, but does not prove rigorously, that we also get convergence for $p=1$.

A similar argument suggests that the sum diverges for $p=0$.

\beginscanswers{9}

\hwsolnhdr{fourier-ish-integral}
First we rewrite the integrand as
\begin{align*}
  & \frac{1}{4}\left(e^{ix}+e^{-ix}\right)\left(e^{2ix}+2^{-2ix}\right) \\
  &= \frac{1}{4}\left(e^{3ix}+e^{-3ix}+e^{ix}+e^{-ix}\right)\eqquad.
\end{align*}
The indefinite integral is
\begin{equation*}
  \frac{1}{12i}\left(e^{3ix}-e^{-3ix}\right)+\frac{1}{4i}\left(e^{ix}-e^{-ix}\right)
\end{equation*}
Evaluating this at 0 gives 0, while at $\pi/2$ we find 1/3. The result is 1/3.

\hwsolnhdr{addition-theorem-for-sine}
\begin{align*}
\sin(a+b) &= \left(e^{i(a+b)}-e^{-i(a+b)}\right)/2i \\
          &= \left(e^{ia}e^{ib}-e^{-ia}e^{-ib}\right)/2i \\
          &= \left[(\cos a+i\sin a)(\cos b+i\sin b)-(\cos a-i\sin a)(\cos b-i\sin b)\right]/2i \\
          &= \left[(\cos a+i\sin a)(\cos b+i\sin b)-(\cos a-i\sin a)(\cos b-i\sin b)\right]/2i \\
          &= \cos a\sin b +\sin a\cos b 
\end{align*}
By a similar computation, we find $\cos(a+b)=\cos a\cos b-\sin a\sin b$.

\hwsolnhdr{cube-roots-of-unity}
If $z^3=1$, then we know that $|z|=1$, since cubing $z$ cubes its magnitude. Cubing $z$ triples
its argument, so the argument of $z$ must be a number that, when tripled, is equivalent to an
angle of zero. There are three possibilities: $0\times 3=0$, $(2\pi/3)\times 3=2\pi$,
and $(4\pi/3)\times 3=4\pi$. (Other possibilities, such as $(32\pi/3)$, are equivalent to
one of these.) The solutions are:
\begin{equation*}
z = 1,\ e^{2\pi i/3},\ e^{4\pi i/3}
\end{equation*}

\hwsolnhdr{factor-cubic}
We can think of this as a polynomial in $x$ or a polynomial in $y$ --- their roles are symmetric. Let's call $x$ the variable.
By the fundamental theorem of algebra, it must be possible to factor it into a product of three
linear factors, if the coefficients are allowed to be complex. Each of these factors causes the
product to be zero for a certain value of $x$. But the condition for the expression to be
zero is $x^3=y^3$, which basically means that the ratio of $x$ to $y$ must be a third root of 1.
The problem, then, boils down to finding the three third roots of 1, as in
problem \ref{hw:cube-roots-of-unity}. Using the result of that problem, we find that there
are zeroes when $x/y$ equals $1$, $e^{2\pi i/3}$, and $e^{4\pi i/3}$. This tells us that
the factorization is $(x-y)(x-e^{2\pi i/3}y)(x-e^{4\pi i/3}y)$.

The second part of the problem asks us to factorize as much as possible using real coefficients.
Our only hope of doing this is to multiply out the two factors that involve complex coefficients,
and see if they produce something real. In fact, we can anticipate that it will work, because
the coefficients are complex conjugates of one another, and when a quadratic has two complex
roots, they are conjugates. The result is $(x-y)(x^2+xy+y^2)$.

\hwsolnhdr{diffeq-with-composition}
Applying the differential equation to the form suggested gives $abx^{b-1}=a^{b+1}x^{b^2}$.
The exponents must be equal on both sides, so $b$ must be a solution of $b^2-b+1$. The solutions
are $b=(1\pm\sqrt{3}i)/2$. For a more detailed discussion of this cute problem, see
\url{mathoverflow.net/questions/111066}.

\hwsolnhdr{partial-fraction-high-power}
(a) Let $m=10,000$. We know that integrals of this form can be done, at least in theory, using partial fractions.
The ten thousand roots of the polynomial will be ten thousand points evenly spaced around
the unit circle in the complex plane. They can be expressed as $r_k=e^{2\pi k/m}$ for $k=0$ to $m-1$.
Since all the roots are unequal, the partial-fraction form of the integrand contains only terms
of the form $A_k/(x-r_k)$. Integrating, we would get a sum of ten thousand terms of the form
$A_k\ln(x-r_k)$.\\
(b) I tried inputting the integral into three different pieces of symbolic
math software: the open-source packages Yacas and Maxima, and the web-based interface to
Wolfram's proprietary Mathematica software at integrals.com. Maxima gave a partially integrated result after a couple
of minutes of computation. Yacas crashed. Mathematica's web interface timed out and suggested
buying a stand-alone copy of Mathematica. All three programs probably embarked on the computation
of the $A_k$ by attempting to solve 10,000 equations in the 10,000 unknowns $A_k$, and then ran out
of resources (either memory or CPU time).\\
(c) The expressions look nicer if we let $\omega=e^{2\pi/m}$, so that $r_k=\omega^k$. The residue method gives
\begin{equation*}
  \frac{1}{x^m-1} = \sum \frac{1}{(x-\omega^k)m \omega^{k(m-1)}}\eqquad.
\end{equation*}
Integration gives
\begin{equation*}
  \int \frac{\der x}{x^m-1} = \sum \frac{1}{m \omega^{k(m-1)}}\ln\left(x-\omega^k\right)\eqquad.
\end{equation*}
(Thanks to math.stackexchage.com user zulon for suggesting the residue mathod, and to
Robert Israel for pointing out that for $|x|<1$ this can also be expressed as a hypergeometric function:
$(- x)\ {}_2F_1\left(\frac{1}{m},1; 1+\frac{1}{m}; x^m\right)$.)
